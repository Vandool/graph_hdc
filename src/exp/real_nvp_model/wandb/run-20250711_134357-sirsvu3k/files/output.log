Running experiment
{ 'activation': <class 'torch.nn.modules.activation.ReLU'>,
  'batch_size': 16,
  'dataset': <SupportedDataset.ZINC_NODE_DEGREE_COMB: 'ZINC_ND_COMB'>,
  'device': 'cuda',
  'dropout_probability': 0.0,
  'epochs': 30,
  'flow_type': <class 'normflows.flows.neural_spline.wrapper.AutoregressiveRationalQuadraticSpline'>,
  'hv_dim': 6400,
  'init_identity': False,
  'input_shape': None,
  'lr': 1.001019515276401,
  'num_bins': 8,
  'num_blocks': 2,
  'num_context_channels': None,
  'num_flows': 16,
  'num_hidden_channels': 128,
  'num_input_channels': 19200,
  'permute': False,
  'project_dir': PosixPath('/home/ka/ka_iti/ka_zi9629/projects/graph_hdc'),
  'seed': 42,
  'tail_bound': 3,
  'vsa': <VSAModel.HRR: 'HRR'>,
  'weight_decay': 0.0}
Setting up experiment in /pfs/data6/home/ka/ka_iti/ka_zi9629/projects/graph_hdc/src/exp/real_nvp_model/results/01_test_train_real_nvp
Experiment directory created: /pfs/data6/home/ka/ka_iti/ka_zi9629/projects/graph_hdc/src/exp/real_nvp_model/results/01_test_train_real_nvp/2025-07-11_13-43-58_uvcj
Saved a copy of the script to /pfs/data6/home/ka/ka_iti/ka_zi9629/projects/graph_hdc/src/exp/real_nvp_model/results/01_test_train_real_nvp/2025-07-11_13-43-58_uvcj/01_test_train_real_nvp.py
CUDA is available. Detected 1 GPU device.
Loading existing HyperNet from /pfs/data6/home/ka/ka_iti/ka_zi9629/projects/graph_hdc/_models/hypernet_HRR_d6400_s42_dpth3.pt
Loading existing PCA from /pfs/data6/home/ka/ka_iti/ka_zi9629/projects/graph_hdc/_models/hypervec_pca_HRR_d6400_s42_c998.joblib
Loaded PCA with 113 components
You have turned on `Trainer(detect_anomaly=True)`. This will significantly slow down compute speed and is recommended only for model debugging.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
You are using a CUDA device ('NVIDIA H100') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name | Type            | Params | Mode
-------------------------------------------------
0 | flow | NormalizingFlow | 5.6 M  | train
-------------------------------------------------
5.6 M     Trainable params
0         Non-trainable params
5.6 M     Total params
22.336    Total estimated model params size (MB)
323       Modules in train mode
0         Modules in eval mode
SLURM auto-requeueing enabled. Setting signal handlers.
Epoch 0:  25%|██▌       | 1/4 [00:00<00:02,  1.43it/s, v_num=vu3k, train_loss_step=1.97e+8]Skipping NaN/Inf loss at step 1
/pfs/data6/home/ka/ka_iti/ka_zi9629/projects/graph_hdc/.pixi/envs/default/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=20). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
/pfs/data6/home/ka/ka_iti/ka_zi9629/projects/graph_hdc/.pixi/envs/default/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:  50%|█████     | 2/4 [00:01<00:01,  1.77it/s, v_num=vu3k, train_loss_step=1.97e+8]Skipping NaN/Inf loss at step 2
Epoch 0:  75%|███████▌  | 3/4 [00:01<00:00,  1.92it/s, v_num=vu3k, train_loss_step=1.97e+8]Skipping NaN/Inf loss at step 3
Epoch 1:   0%|          | 0/4 [00:00<?, ?it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 0
Epoch 1:  25%|██▌       | 1/4 [00:00<00:01,  1.56it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 1
Epoch 1:  50%|█████     | 2/4 [00:01<00:01,  1.87it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 2
Epoch 1:  75%|███████▌  | 3/4 [00:01<00:00,  2.00it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 3
Epoch 2:   0%|          | 0/4 [00:00<?, ?it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 0
Epoch 2:  25%|██▌       | 1/4 [00:00<00:01,  1.57it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 1
Epoch 2:  50%|█████     | 2/4 [00:01<00:01,  1.87it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 2
Epoch 2:  75%|███████▌  | 3/4 [00:01<00:00,  2.00it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 3
Epoch 3:   0%|          | 0/4 [00:00<?, ?it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 0
Epoch 3:  25%|██▌       | 1/4 [00:00<00:01,  1.56it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 1
Epoch 3:  50%|█████     | 2/4 [00:01<00:01,  1.85it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 2
Epoch 3:  75%|███████▌  | 3/4 [00:01<00:00,  1.98it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 3
Epoch 4:   0%|          | 0/4 [00:00<?, ?it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 0
Epoch 4:  25%|██▌       | 1/4 [00:00<00:01,  1.59it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 1
Epoch 4:  50%|█████     | 2/4 [00:01<00:01,  1.89it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 2
Epoch 4:  75%|███████▌  | 3/4 [00:01<00:00,  2.02it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 3
Epoch 5:   0%|          | 0/4 [00:00<?, ?it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 0
Epoch 5:  25%|██▌       | 1/4 [00:00<00:01,  1.57it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 1
Epoch 5:  50%|█████     | 2/4 [00:01<00:01,  1.87it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 2
Epoch 5:  75%|███████▌  | 3/4 [00:01<00:00,  2.00it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 3
Epoch 6:   0%|          | 0/4 [00:00<?, ?it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 0
Epoch 6:  25%|██▌       | 1/4 [00:00<00:01,  1.57it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 1
Epoch 6:  50%|█████     | 2/4 [00:01<00:01,  1.88it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 2
Epoch 6:  75%|███████▌  | 3/4 [00:01<00:00,  2.01it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 3
Epoch 7:   0%|          | 0/4 [00:00<?, ?it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 0
Epoch 7:  25%|██▌       | 1/4 [00:00<00:01,  1.55it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 1
Epoch 7:  50%|█████     | 2/4 [00:01<00:01,  1.86it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 2
Epoch 7:  75%|███████▌  | 3/4 [00:01<00:00,  1.99it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 3
Epoch 8:   0%|          | 0/4 [00:00<?, ?it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 0
Epoch 8:  25%|██▌       | 1/4 [00:00<00:01,  1.57it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 1
Epoch 8:  50%|█████     | 2/4 [00:01<00:01,  1.87it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 2
Epoch 8:  75%|███████▌  | 3/4 [00:01<00:00,  2.00it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 3
Epoch 9:   0%|          | 0/4 [00:00<?, ?it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 0
Epoch 9:  25%|██▌       | 1/4 [00:00<00:01,  1.57it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 1
Epoch 9:  50%|█████     | 2/4 [00:01<00:01,  1.88it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 2
Epoch 9:  75%|███████▌  | 3/4 [00:01<00:00,  2.01it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 3
Epoch 10:   0%|          | 0/4 [00:00<?, ?it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 0
Epoch 10:  25%|██▌       | 1/4 [00:00<00:01,  1.56it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 1
Epoch 10:  50%|█████     | 2/4 [00:01<00:01,  1.87it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 2
Epoch 10:  75%|███████▌  | 3/4 [00:01<00:00,  2.00it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 3
Epoch 11:   0%|          | 0/4 [00:00<?, ?it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 0
Epoch 11:  25%|██▌       | 1/4 [00:00<00:01,  1.58it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 1
Epoch 11:  50%|█████     | 2/4 [00:01<00:01,  1.88it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 2
Epoch 11:  75%|███████▌  | 3/4 [00:01<00:00,  2.01it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 3
Epoch 12:   0%|          | 0/4 [00:00<?, ?it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 0
Epoch 12:  25%|██▌       | 1/4 [00:00<00:01,  1.54it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 1
Epoch 12:  50%|█████     | 2/4 [00:01<00:01,  1.85it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 2
Epoch 12:  75%|███████▌  | 3/4 [00:01<00:00,  1.99it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 3
Epoch 13:   0%|          | 0/4 [00:00<?, ?it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 0
Epoch 13:  25%|██▌       | 1/4 [00:00<00:01,  1.55it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 1
Epoch 13:  50%|█████     | 2/4 [00:01<00:01,  1.86it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 2
Epoch 13:  75%|███████▌  | 3/4 [00:01<00:00,  1.99it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 3
Epoch 14:   0%|          | 0/4 [00:00<?, ?it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 0
Epoch 14:  25%|██▌       | 1/4 [00:00<00:01,  1.56it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 1
Epoch 14:  50%|█████     | 2/4 [00:01<00:01,  1.87it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 2
Epoch 14:  75%|███████▌  | 3/4 [00:01<00:00,  2.00it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 3
Epoch 15:   0%|          | 0/4 [00:00<?, ?it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 0
Epoch 15:  25%|██▌       | 1/4 [00:00<00:01,  1.56it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 1
Epoch 15:  50%|█████     | 2/4 [00:01<00:01,  1.87it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 2
Epoch 15:  75%|███████▌  | 3/4 [00:01<00:00,  2.00it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 3
Epoch 16:   0%|          | 0/4 [00:00<?, ?it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 0
Epoch 16:  25%|██▌       | 1/4 [00:00<00:01,  1.56it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 1
Epoch 16:  50%|█████     | 2/4 [00:01<00:01,  1.87it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 2
Epoch 16:  75%|███████▌  | 3/4 [00:01<00:00,  2.00it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 3
Epoch 17:   0%|          | 0/4 [00:00<?, ?it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 0
Epoch 17:  25%|██▌       | 1/4 [00:00<00:01,  1.57it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 1
Epoch 17:  50%|█████     | 2/4 [00:01<00:01,  1.87it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 2
Epoch 17:  75%|███████▌  | 3/4 [00:01<00:00,  2.00it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 3
Epoch 18:   0%|          | 0/4 [00:00<?, ?it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 0
Epoch 18:  25%|██▌       | 1/4 [00:00<00:01,  1.60it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 1
Epoch 18:  50%|█████     | 2/4 [00:01<00:01,  1.90it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 2
Epoch 18:  75%|███████▌  | 3/4 [00:01<00:00,  2.02it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 3
Epoch 19:   0%|          | 0/4 [00:00<?, ?it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 0
Epoch 19:  25%|██▌       | 1/4 [00:00<00:01,  1.57it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 1
Epoch 19:  50%|█████     | 2/4 [00:01<00:01,  1.87it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 2
Epoch 19:  75%|███████▌  | 3/4 [00:01<00:00,  2.00it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 3
Epoch 20:   0%|          | 0/4 [00:00<?, ?it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 0
Epoch 20:  25%|██▌       | 1/4 [00:00<00:01,  1.56it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 1
Epoch 20:  50%|█████     | 2/4 [00:01<00:01,  1.87it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 2
Epoch 20:  75%|███████▌  | 3/4 [00:01<00:00,  2.00it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 3
Epoch 21:   0%|          | 0/4 [00:00<?, ?it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 0
Epoch 21:  25%|██▌       | 1/4 [00:00<00:01,  1.60it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 1
Epoch 21:  50%|█████     | 2/4 [00:01<00:01,  1.89it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 2
Epoch 21:  75%|███████▌  | 3/4 [00:01<00:00,  2.02it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 3
Epoch 22:   0%|          | 0/4 [00:00<?, ?it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 0
Epoch 22:  25%|██▌       | 1/4 [00:00<00:01,  1.56it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 1
Epoch 22:  50%|█████     | 2/4 [00:01<00:01,  1.87it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 2
Epoch 22:  75%|███████▌  | 3/4 [00:01<00:00,  2.00it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 3
Epoch 23:   0%|          | 0/4 [00:00<?, ?it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 0
Epoch 23:  25%|██▌       | 1/4 [00:00<00:01,  1.56it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 1
Epoch 23:  50%|█████     | 2/4 [00:01<00:01,  1.86it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 2
Epoch 23:  75%|███████▌  | 3/4 [00:01<00:00,  1.99it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 3
Epoch 24:   0%|          | 0/4 [00:00<?, ?it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 0
Epoch 24:  25%|██▌       | 1/4 [00:00<00:01,  1.58it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 1
Epoch 24:  50%|█████     | 2/4 [00:01<00:01,  1.88it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 2
Epoch 24:  75%|███████▌  | 3/4 [00:01<00:00,  2.01it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 3
Epoch 25:   0%|          | 0/4 [00:00<?, ?it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 0
Epoch 25:  25%|██▌       | 1/4 [00:00<00:01,  1.54it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 1
Epoch 25:  50%|█████     | 2/4 [00:01<00:01,  1.85it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 2
Epoch 25:  75%|███████▌  | 3/4 [00:01<00:00,  1.99it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 3
Epoch 26:   0%|          | 0/4 [00:00<?, ?it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 0
Epoch 26:  25%|██▌       | 1/4 [00:00<00:01,  1.58it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 1
Epoch 26:  50%|█████     | 2/4 [00:01<00:01,  1.88it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 2
Epoch 26:  75%|███████▌  | 3/4 [00:01<00:00,  2.01it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 3
Epoch 27:   0%|          | 0/4 [00:00<?, ?it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 0
Epoch 27:  25%|██▌       | 1/4 [00:00<00:01,  1.58it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 1
Epoch 27:  50%|█████     | 2/4 [00:01<00:01,  1.88it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 2
Epoch 27:  75%|███████▌  | 3/4 [00:01<00:00,  2.01it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 3
Epoch 28:   0%|          | 0/4 [00:00<?, ?it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 0
Epoch 28:  25%|██▌       | 1/4 [00:00<00:01,  1.58it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 1
Epoch 28:  50%|█████     | 2/4 [00:01<00:01,  1.89it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 2
Epoch 28:  75%|███████▌  | 3/4 [00:01<00:00,  2.01it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 3
Epoch 29:   0%|          | 0/4 [00:00<?, ?it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 0
Epoch 29:  25%|██▌       | 1/4 [00:00<00:01,  1.57it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 1
Epoch 29:  50%|█████     | 2/4 [00:01<00:01,  1.87it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 2
Epoch 29:  75%|███████▌  | 3/4 [00:01<00:00,  2.00it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]Skipping NaN/Inf loss at step 3
Epoch 29: 100%|██████████| 4/4 [00:02<00:00,  1.78it/s, v_num=vu3k, train_loss_step=1.97e+8, val_loss=nan.0, lr=1.000, train_loss_epoch=1.97e+8]
Saved train/val loss plot to /pfs/data6/home/ka/ka_iti/ka_zi9629/projects/graph_hdc/src/exp/real_nvp_model/results/01_test_train_real_nvp/2025-07-11_13-43-58_uvcj/artefacts/train_val_loss.png
`Trainer.fit` stopped: `max_epochs=30` reached.
==== The Experiment is done! ====
