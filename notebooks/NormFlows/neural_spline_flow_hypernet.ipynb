{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T06:46:35.084783Z",
     "start_time": "2025-07-15T06:46:14.413863Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.encoding.the_types import VSAModel\n",
    "# hyperflow_pipeline.py\n",
    "\n",
    "# ----------------------------\n",
    "# 1. Imports\n",
    "# ----------------------------\n",
    "import numpy as np\n",
    "import normflows as nf\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from torchhd import HRRTensor\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch_geometric.datasets import ZINC\n",
    "from src.datasets import AddNodeDegree\n",
    "from torch_geometric import loader\n",
    "\n",
    "from src.encoding.configs_and_constants import SupportedDataset\n",
    "import torch\n",
    "\n",
    "from src.encoding.graph_encoders import HyperNet\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 2. Dataset + DataLoader\n",
    "# ----------------------------\n",
    "dataset = ZINC(\n",
    "    root=\"/Users/arvandkaveh/Projects/kit/graph_hdc/datasets/test/zinc_subset_with_degree\",\n",
    "    pre_transform=AddNodeDegree()\n",
    ")\n",
    "\n",
    "batch_size = 32\n",
    "dataloader = loader.DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 3. VSA / configuration\n",
    "# ----------------------------\n",
    "vsa = \"HRR\"\n",
    "hv_dim = 8 * 8\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "seed = 42\n",
    "\n",
    "ds = SupportedDataset.ZINC_NODE_DEGREE_COMB\n",
    "ds.default_cfg.vsa = VSAModel(vsa)\n",
    "ds.default_cfg.hv_dim = hv_dim\n",
    "ds.default_cfg.device = device\n",
    "ds.default_cfg.seed = seed\n",
    "\n",
    "# Disable edge and graph features\n",
    "ds.default_cfg.edge_feature_configs = {}\n",
    "ds.default_cfg.graph_feature_configs = {}\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 4. HyperNet encoder helper\n",
    "# ----------------------------\n",
    "dataloader_iter = iter(dataloader)\n",
    "hypernet = HyperNet(\n",
    "    config=ds.default_cfg,\n",
    "    depth=3\n",
    ")\n",
    "\n",
    "def get_my_hypervectors():\n",
    "    n = next(dataloader_iter)\n",
    "    res = hypernet.forward(data=n)\n",
    "    x = torch.stack(\n",
    "        [res[\"node_terms\"], res[\"edge_terms\"], res[\"graph_embedding\"]],\n",
    "        dim=1\n",
    "    )\n",
    "    return x\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 5. Build normalizing flow\n",
    "# ----------------------------\n",
    "K = 16\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "latent_size = 3 * hv_dim\n",
    "hidden_units = 128\n",
    "hidden_layers = 2\n",
    "\n",
    "flows = []\n",
    "for i in range(K):\n",
    "    flows.append(\n",
    "        nf.flows.AutoregressiveRationalQuadraticSpline(\n",
    "            latent_size, hidden_layers, hidden_units\n",
    "        )\n",
    "    )\n",
    "    flows.append(\n",
    "        nf.flows.LULinearPermute(latent_size)\n",
    "    )\n",
    "\n",
    "# Base distribution\n",
    "q0 = nf.distributions.DiagGaussian(latent_size, trainable=False)\n",
    "\n",
    "# Construct the flow\n",
    "nfm = nf.NormalizingFlow(q0=q0, flows=flows)\n",
    "nfm = nfm.to(device)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 6. Train\n",
    "# ----------------------------\n",
    "max_iter = 400\n",
    "show_iter = 100\n",
    "lr = 1e-3\n",
    "wd = 1e-5\n",
    "\n",
    "loss_hist = np.array([])\n",
    "\n",
    "nfm.train()\n",
    "optimizer = torch.optim.Adam(nfm.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "for it in tqdm(range(max_iter)):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # get a batch of [batch_size, 3, D]\n",
    "    x = get_my_hypervectors()\n",
    "    x = x.to(device).view(batch_size, -1)  # flatten to [batch_size, 3*D]\n",
    "\n",
    "    # Compute loss\n",
    "    loss = nfm.forward_kld(x)\n",
    "\n",
    "    # Backprop + step\n",
    "    if not (torch.isnan(loss) or torch.isinf(loss)):\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Log\n",
    "    loss_hist = np.append(loss_hist, loss.to('cpu').item())\n",
    "\n",
    "    if (it + 1) % show_iter == 0:\n",
    "        print(f\"iter {it+1}, loss {loss:.4f}\")\n",
    "\n",
    "# Plot loss curve\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(loss_hist, label='loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 7. Sampling\n",
    "# ----------------------------\n",
    "n_samples = 10\n",
    "nfm.eval()\n",
    "s, _ = nfm.sample(n_samples)\n",
    "z = s.to(\"cpu\")                     # [n_samples, 3*D]\n",
    "z = z.view(n_samples, 3, hv_dim)    # [n_samples, 3, D]\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 8. Cast to HRRTensor\n",
    "# ----------------------------\n",
    "node_terms_s, edge_terms_s, graph_embeddings_s = z.unbind(dim=1)\n",
    "\n",
    "node_terms_s_hrr = node_terms_s.as_subclass(HRRTensor)\n",
    "edge_terms_s_hrr = edge_terms_s.as_subclass(HRRTensor)\n",
    "graph_embeddings_s_hrr = graph_embeddings_s.as_subclass(HRRTensor)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 9. Reconstruction loop\n",
    "# ----------------------------\n",
    "for b in range(n_samples):\n",
    "    data_dec, node_counter_dec, edge_counter_dec = hypernet.reconstruct(\n",
    "        graph_hv=graph_embeddings_s_hrr[b],\n",
    "        node_terms=node_terms_s_hrr[b],\n",
    "        edge_terms=edge_terms_s_hrr[b],\n",
    "        learning_rate=0.1,\n",
    "        batch_size=10,\n",
    "        low=0,\n",
    "        high=1,\n",
    "        alpha=0.1,\n",
    "        lambda_l1=0.01,\n",
    "        use_node_degree=True  # alpha=0 → zeroes the node_degree loss\n",
    "    )\n",
    "\n",
    "    print(f\"data_dec: {data_dec.shape}\")\n",
    "    print(f\"node_counter_dec: {node_counter_dec}\")\n",
    "    print(f\"edge_counter_dec: {edge_counter_dec}\")"
   ],
   "id": "2721747a7687dd9b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 99/400 [00:09<00:28, 10.69it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported format string passed to HRRTensor.__format__",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mTypeError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 140\u001B[39m\n\u001B[32m    137\u001B[39m     loss_hist = np.append(loss_hist, loss.to(\u001B[33m'\u001B[39m\u001B[33mcpu\u001B[39m\u001B[33m'\u001B[39m).item())\n\u001B[32m    139\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m (it + \u001B[32m1\u001B[39m) % show_iter == \u001B[32m0\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m140\u001B[39m         \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33miter \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mit+\u001B[32m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m, loss \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mloss\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m    142\u001B[39m \u001B[38;5;66;03m# Plot loss curve\u001B[39;00m\n\u001B[32m    143\u001B[39m plt.figure(figsize=(\u001B[32m10\u001B[39m, \u001B[32m10\u001B[39m))\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Projects/kit/graph_hdc/.pixi/envs/default/lib/python3.13/site-packages/torch/_tensor.py:1095\u001B[39m, in \u001B[36mTensor.__format__\u001B[39m\u001B[34m(self, format_spec)\u001B[39m\n\u001B[32m   1093\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__format__\u001B[39m(\u001B[38;5;28mself\u001B[39m, format_spec):\n\u001B[32m   1094\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m-> \u001B[39m\u001B[32m1095\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mhandle_torch_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43mTensor\u001B[49m\u001B[43m.\u001B[49m\u001B[34;43m__format__\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mformat_spec\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1096\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.dim() == \u001B[32m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m.is_meta \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;129;01mis\u001B[39;00m Tensor:\n\u001B[32m   1097\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.item().\u001B[34m__format__\u001B[39m(format_spec)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Projects/kit/graph_hdc/.pixi/envs/default/lib/python3.13/site-packages/torch/overrides.py:1742\u001B[39m, in \u001B[36mhandle_torch_function\u001B[39m\u001B[34m(public_api, relevant_args, *args, **kwargs)\u001B[39m\n\u001B[32m   1734\u001B[39m     warnings.warn(\n\u001B[32m   1735\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mDefining your `__torch_function__ as a plain method is deprecated and \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1736\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mwill be an error in future, please define it as a classmethod.\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m   1737\u001B[39m         \u001B[38;5;167;01mDeprecationWarning\u001B[39;00m,\n\u001B[32m   1738\u001B[39m     )\n\u001B[32m   1740\u001B[39m \u001B[38;5;66;03m# Use `public_api` instead of `implementation` so __torch_function__\u001B[39;00m\n\u001B[32m   1741\u001B[39m \u001B[38;5;66;03m# implementations can do equality/identity comparisons.\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1742\u001B[39m result = \u001B[43mtorch_func_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpublic_api\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtypes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1744\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m result \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mNotImplemented\u001B[39m:\n\u001B[32m   1745\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Projects/kit/graph_hdc/.pixi/envs/default/lib/python3.13/site-packages/torch/_tensor.py:1648\u001B[39m, in \u001B[36mTensor.__torch_function__\u001B[39m\u001B[34m(cls, func, types, args, kwargs)\u001B[39m\n\u001B[32m   1645\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mNotImplemented\u001B[39m\n\u001B[32m   1647\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m _C.DisableTorchFunctionSubclass():\n\u001B[32m-> \u001B[39m\u001B[32m1648\u001B[39m     ret = \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1649\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m func \u001B[38;5;129;01min\u001B[39;00m get_default_nowrap_functions():\n\u001B[32m   1650\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m ret\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Projects/kit/graph_hdc/.pixi/envs/default/lib/python3.13/site-packages/torch/_tensor.py:1098\u001B[39m, in \u001B[36mTensor.__format__\u001B[39m\u001B[34m(self, format_spec)\u001B[39m\n\u001B[32m   1096\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.dim() == \u001B[32m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m.is_meta \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;129;01mis\u001B[39;00m Tensor:\n\u001B[32m   1097\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.item().\u001B[34m__format__\u001B[39m(format_spec)\n\u001B[32m-> \u001B[39m\u001B[32m1098\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mobject\u001B[39;49m\u001B[43m.\u001B[49m\u001B[34;43m__format__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mformat_spec\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mTypeError\u001B[39m: unsupported format string passed to HRRTensor.__format__"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "7609bb65e92b3c46",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
