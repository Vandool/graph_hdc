{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-02T18:48:00.121910Z",
     "start_time": "2025-06-02T18:47:57.582153Z"
    }
   },
   "source": [
    "from pathlib import Path\n",
    "from src.datasets import AddNodeDegree\n",
    "\n",
    "root = Path.cwd()\n",
    "project_dir = root.resolve().parent\n",
    "datasets = project_dir / \"datasets\""
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T18:49:28.734419Z",
     "start_time": "2025-06-02T18:48:51.461744Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.datasets import QM9\n",
    "\n",
    "\n",
    "dataset = QM9(root=datasets / 'test' / 'QM9D',pre_transform=AddNodeDegree())\n",
    "loader =DataLoader(dataset, batch_size=3, shuffle=False)\n",
    "n = 1\n",
    "for batch in loader:\n",
    "    if n > 0:\n",
    "        n -= 1\n",
    "    else:\n",
    "        break\n",
    "    # batch.x: [N_total, num_features]\n",
    "    # batch.batch: [N_total] where batch[i]=g means node i belongs to graph g (0 ≤ g < batch_size)\n",
    "    print(batch.x.shape, batch.batch.shape)\n",
    "    # e.g. torch.Size([35, 11]) torch.Size([35])\n",
    "print(len(dataset))\n",
    "print(dataset[0])\n",
    "d = dataset[0]\n",
    "d2 = dataset[4]\n",
    "d.smiles()"
   ],
   "id": "a99103d22f130415",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "100%|██████████| 133885/133885 [00:33<00:00, 3952.11it/s]\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 12]) torch.Size([12])\n",
      "130831\n",
      "Data(x=[5, 12], edge_index=[2, 8], edge_attr=[8, 4], y=[1, 19], pos=[5, 3], z=[5], smiles='[H]C([H])([H])[H]', name='gdb_1', idx=[1])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'str' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mTypeError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[3]\u001B[39m\u001B[32m, line 21\u001B[39m\n\u001B[32m     19\u001B[39m d = dataset[\u001B[32m0\u001B[39m]\n\u001B[32m     20\u001B[39m d2 = dataset[\u001B[32m4\u001B[39m]\n\u001B[32m---> \u001B[39m\u001B[32m21\u001B[39m \u001B[43md\u001B[49m\u001B[43m.\u001B[49m\u001B[43msmiles\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mTypeError\u001B[39m: 'str' object is not callable"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Utility to gather unique values per feature index\n",
    "def gather_unique_values(dataset, attr_name, feature_dim):\n",
    "    uniques = defaultdict(set)\n",
    "    for data in dataset:\n",
    "        feat = getattr(data, attr_name)\n",
    "        # Handle 1D or 2D feature arrays\n",
    "        if feat is None:\n",
    "            continue\n",
    "        feat = feat.detach()\n",
    "        if feat.dim() == 2:\n",
    "            for i in range(feat.size(1)):\n",
    "                uniques[i].update(feat[:, i].unique().tolist())\n",
    "        elif feat.dim() == 1 and feature_dim is None:\n",
    "            uniques[0].update(feat.unique().tolist())\n",
    "        else:\n",
    "            # treat entire tensor as one feature (e.g., for y)\n",
    "            uniques[0].update(feat.view(-1).unique().tolist())\n",
    "    return uniques\n",
    "\n",
    "# Node features: data.x has shape [total_nodes, 11]\n",
    "node_uniques = gather_unique_values(dataset, 'x', feature_dim=dataset.num_node_features)\n",
    "print(\"Node feature categories per index:\")\n",
    "for idx, vals in sorted(node_uniques.items()):\n",
    "    print(f\"  - Feature {idx}: {len(vals)} distinct values → {sorted(vals)}\")\n",
    "\n",
    "# Edge features: data.edge_attr has shape [total_edges, 4]\n",
    "edge_uniques = gather_unique_values(dataset, 'edge_attr', feature_dim=dataset.num_edge_features)\n",
    "print(\"\\nEdge feature categories per index:\")\n",
    "for idx, vals in sorted(edge_uniques.items()):\n",
    "    print(f\"  - Feature {idx}: {len(vals)} distinct values → {sorted(vals)}\")\n",
    "\n",
    "# Graph‐level targets: data.y has shape [1, 19]\n",
    "# Here we treat each of the 19 targets as one \"feature column\":\n",
    "graph_uniques = defaultdict(set)\n",
    "for data in dataset:\n",
    "    y = data.y.squeeze()  # shape [19]\n",
    "    for i, val in enumerate(y.tolist()):\n",
    "        graph_uniques[i].add(val)\n",
    "print(\"\\nGraph‐level target categories per index:\")\n",
    "for idx, vals in sorted(graph_uniques.items()):\n",
    "    print(f\"  - Target {idx}: {len(vals)} distinct values\")  # continuous, so this will be size = num_graphs\n",
    "\n",
    "# Additionally, checking atomic numbers (data.z) and hybridization categories:\n",
    "z_uniques = gather_unique_values(dataset, 'z', feature_dim=None)\n",
    "print(\"\\nAtomic number categories:\", sorted(z_uniques[0]))"
   ],
   "id": "ee0d16b275b79995",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
