{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.data import Dataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "from src.encoding.configs_and_constants import ZINC_SMILES_HRR_1024_F64_5G1NG4_CONFIG, ZINC_SMILES_HRR_7744_CONFIG_F64\n",
    "from src.encoding.graph_encoders import load_or_create_hypernet\n",
    "from src.utils.utils import GLOBAL_MODEL_PATH\n",
    "\n",
    "\n",
    "def analyze_dataset(ds: Dataset, name: str = \"\", feature_attr: str = \"x\") -> dict:\n",
    "    \"\"\"\n",
    "    Compute statistics for a PyG dataset with 4-tuple node features\n",
    "    and draw histograms for node counts, edge counts, and node-type distribution.\n",
    "    \"\"\"\n",
    "    num_nodes_list, num_edges_list = [], []\n",
    "    node_type_counter = Counter()\n",
    "    logp_list: list[float] = []  # <-- collect logP if available\n",
    "\n",
    "    for data in ds:\n",
    "        n = int(data.num_nodes)\n",
    "        e = int(data.num_edges) // 2  # dataset is undirected\n",
    "        num_nodes_list.append(n)\n",
    "        num_edges_list.append(e)\n",
    "\n",
    "        x: torch.Tensor = getattr(data, feature_attr)\n",
    "        if x.dim() != 2 or x.size(1) != 4:\n",
    "            raise ValueError(f\"Expected {feature_attr} shape [N, 4], got {tuple(x.shape)}\")\n",
    "        rows = x.detach().cpu().to(torch.int64).tolist()\n",
    "        node_type_counter.update(map(tuple, rows))\n",
    "\n",
    "        # --- logP (optional) ---\n",
    "        if hasattr(data, \"logp\"):\n",
    "            lp = data.logp\n",
    "            # accept scalar tensor or 1-element tensor\n",
    "            if isinstance(lp, torch.Tensor):\n",
    "                lp = float(lp.detach().cpu().reshape(-1)[0])\n",
    "            else:\n",
    "                lp = float(lp)\n",
    "            logp_list.append(lp)\n",
    "\n",
    "    def stats(arr):\n",
    "        arr = np.asarray(arr)\n",
    "        return {\n",
    "            \"min\": int(arr.min()),\n",
    "            \"max\": int(arr.max()),\n",
    "            \"mean\": float(arr.mean()),\n",
    "            \"median\": float(np.median(arr)),\n",
    "            \"std\": float(arr.std(ddof=1)) if len(arr) > 1 else 0.0,\n",
    "        }\n",
    "\n",
    "    summary = {\n",
    "        \"dataset\": name,\n",
    "        \"num_graphs\": len(ds),\n",
    "        \"nodes\": stats(num_nodes_list),\n",
    "        \"edges\": stats(num_edges_list),\n",
    "        \"total_node_types\": len(node_type_counter),\n",
    "        \"node_type_distribution\": dict(node_type_counter),\n",
    "    }\n",
    "\n",
    "    if logp_list:\n",
    "        summary[\"logp\"] = stats(logp_list)  # include logP stats if present\n",
    "\n",
    "\n",
    "    # --- plotting ---\n",
    "    fig = plt.figure(figsize=(16, 10))  # wider and taller canvas\n",
    "    gs = fig.add_gridspec(2, 2, height_ratios=[1, 3])  # bottom row 3x taller\n",
    "\n",
    "    ax_nodes = fig.add_subplot(gs[0, 0])\n",
    "    ax_edges = fig.add_subplot(gs[0, 1])\n",
    "    ax_types = fig.add_subplot(gs[1, :])  # spans the whole width\n",
    "\n",
    "    # Add a little more margin at the bottom so rotated labels fit\n",
    "    plt.subplots_adjust(bottom=0.25)\n",
    "\n",
    "    # Node count distribution\n",
    "    ax_nodes.hist(num_nodes_list, bins=20, color=\"steelblue\", edgecolor=\"black\")\n",
    "    ax_nodes.set_title(f\"{name} – Node count distribution\")\n",
    "    ax_nodes.set_xlabel(\"num_nodes\")\n",
    "    ax_nodes.set_ylabel(\"frequency\")\n",
    "\n",
    "    # Edge count distribution\n",
    "    ax_edges.hist(num_edges_list, bins=40, color=\"darkorange\", edgecolor=\"black\")\n",
    "    ax_edges.set_title(f\"{name} – Edge count distribution\")\n",
    "    ax_edges.set_xlabel(\"num_edges\")\n",
    "    ax_edges.set_ylabel(\"frequency\")\n",
    "\n",
    "    # Node type distribution (wider)\n",
    "    if node_type_counter:\n",
    "        labels, counts = zip(*node_type_counter.most_common())\n",
    "        ax_types.bar(range(len(labels)), counts, color=\"seagreen\", edgecolor=\"black\")\n",
    "        ax_types.set_xticks(range(len(labels)))\n",
    "        ax_types.set_xticklabels([str(l) for l in labels], rotation=90, fontsize=8)\n",
    "    ax_types.set_title(f\"{name} – Node type distribution\")\n",
    "    ax_types.set_xlabel(\"node type (tuple)\")\n",
    "    ax_types.set_ylabel(\"count\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # --- separate logP plot (only if available) ---\n",
    "    if logp_list:\n",
    "        fig2 = plt.figure(figsize=(8, 5))\n",
    "        ax_logp = fig2.add_subplot(1, 1, 1)\n",
    "        ax_logp.hist(logp_list, bins=100, edgecolor=\"black\")\n",
    "        ax_logp.set_title(f\"{name} – logP distribution\")\n",
    "        ax_logp.set_xlabel(\"cLogP (RDKit)\")\n",
    "        ax_logp.set_ylabel(\"frequency\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return summary\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T21:03:13.728021Z",
     "start_time": "2025-11-30T21:02:57.145388Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.datasets.utils import get_split\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "ds = get_split(base_dataset=\"zinc\", split=\"train\")\n",
    "sorted_x, _ = ds.x.sort(dim=0)\n",
    "unique_counts = 1 + (sorted_x[1:, :] != sorted_x[:-1, :]).sum(dim=0)\n",
    "print(unique_counts)\n",
    "\n",
    "ds = get_split(base_dataset=\"qm9\", split=\"train\")\n",
    "sorted_x, _ = ds.x.sort(dim=0)\n",
    "unique_counts = 1 + (sorted_x[1:, :] != sorted_x[:-1, :]).sum(dim=0)\n",
    "print(unique_counts)\n",
    "# stats = analyze_dataset(ds, name=split)\n",
    "# pprint(stats)"
   ],
   "id": "e2ccd0105452c8c4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([9, 5, 3, 4, 2])\n",
      "[QM9:train] filtered 202 disconnected molecules → kept 118677\n",
      "tensor([4, 4, 3, 5])\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torchhd\n",
    "from src.datasets.zinc_smiles_generation import ZincSmiles\n",
    "import torch\n",
    "ds = ZincSmiles(split=\"train\", enc_suffix=)\n",
    "\n",
    "# Number of samples\n",
    "n_samples = 1000\n",
    "\n",
    "# Ensure reproducibility\n",
    "idxs = torch.randperm(len(ds))[:n_samples]\n",
    "device = torch.device(\"cpu\")\n",
    "config = ZINC_SMILES_HRR_7744_CONFIG_F64\n",
    "config.device = device\n",
    "hypernet = load_or_create_hypernet(path=GLOBAL_MODEL_PATH, cfg=ZINC_SMILES_HRR_7744_CONFIG_F64).to(device)\n",
    "\n",
    "for d in [2, 3, 4, 5, 6, 7, 8]:\n",
    "    hypernet.depth = d\n",
    "    batch = next(iter(DataLoader(ds[idxs], batch_size=n_samples)))\n",
    "\n",
    "    graph_terms = hypernet.forward(batch)[\"graph_embedding\"]\n",
    "    print(graph_terms.shape)\n",
    "\n",
    "    # Pairwise cosine similarity matrix [1000, 1000]\n",
    "    cos = torchhd.cos(graph_terms, graph_terms)\n",
    "\n",
    "    # keep only unique, off-diagonal cosine similarities\n",
    "    i, j = torch.triu_indices(cos.size(0), cos.size(1), offset=1)\n",
    "    cs = cos[i, j]  # shape [N*(N-1)/2]\n",
    "\n",
    "    # compute stats\n",
    "    mean_cs = cs.mean().item()\n",
    "    std_cs = cs.std(unbiased=True).item()\n",
    "\n",
    "    print(f\"Mean cosine similarity: {mean_cs:.4f}\")\n",
    "    print(f\"Std  cosine similarity: {std_cs:.4f}\")\n",
    "\n",
    "    # plot\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.hist(cs.cpu().numpy(), bins=50, density=True, alpha=0.75)\n",
    "    plt.xlabel(\"Cosine similarity\")\n",
    "    plt.ylabel(\"Density\")\n",
    "    plt.title(f\"Pairwise cosine similarities (Zinc - MP Depth {d}) \\nμ={mean_cs:.3f}, σ={std_cs:.3f}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "f75bd0a24f313916",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torchhd\n",
    "from src.datasets.zinc_smiles_generation import ZincSmiles\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from src.encoding.configs_and_constants import ZINC_SMILES_HRR_1024_F64_5G1NG4_CONFIG, ZINC_SMILES_HRR_2048_F64_5G1NG4_CONFIG, ZINC_SMILES_HRR_256_F64_5G1NG4_CONFIG\n",
    "\n",
    "# Number of samples\n",
    "n_samples = 1000\n",
    "\n",
    "# Ensure reproducibility\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "for ds_config in [\n",
    "    ZINC_SMILES_HRR_256_F64_5G1NG4_CONFIG,\n",
    "    # ZINC_SMILES_HRR_2048_F64_5G1NG4_CONFIG\n",
    "]:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Dataset Config: {ds_config.name}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "    ds = ZincSmiles(split=\"train\")\n",
    "\n",
    "    idxs = torch.randperm(len(ds))[:n_samples]\n",
    "    ds_config.device = device\n",
    "    hypernet = load_or_create_hypernet(path=GLOBAL_MODEL_PATH, cfg=ds_config).to(device)\n",
    "\n",
    "    for d in [3, 4]:\n",
    "        hypernet.depth = d\n",
    "        batch = next(iter(DataLoader(ds[idxs], batch_size=n_samples)))\n",
    "\n",
    "        graph_terms = hypernet.forward(batch)[\"graph_embedding\"]\n",
    "        print(graph_terms.shape)\n",
    "\n",
    "        # Pairwise cosine similarity matrix [1000, 1000]\n",
    "        cos = torchhd.cos(graph_terms, graph_terms)\n",
    "\n",
    "        # Keep only unique, off-diagonal cosine similarities\n",
    "        i, j = torch.triu_indices(cos.size(0), cos.size(1), offset=1)\n",
    "        cs = cos[i, j]  # shape [N*(N-1)/2]\n",
    "\n",
    "        # Compute Tanimoto distance\n",
    "        # Tanimoto distance = 1 - (A·B) / (||A||² + ||B||² - A·B)\n",
    "        dot_product = torch.mm(graph_terms, graph_terms.t())\n",
    "        norm_sq = torch.sum(graph_terms ** 2, dim=1, keepdim=True)\n",
    "\n",
    "        # Pairwise Tanimoto distances\n",
    "        tanimoto_dist = torch.zeros_like(cos)\n",
    "        for ii in range(graph_terms.size(0)):\n",
    "            for jj in range(graph_terms.size(0)):\n",
    "                dot = dot_product[ii, jj]\n",
    "                denom = norm_sq[ii, 0] + norm_sq[jj, 0] - dot\n",
    "                tanimoto_dist[ii, jj] = 1 - (dot / (denom + 1e-8))\n",
    "\n",
    "        # Extract upper triangle\n",
    "        td = tanimoto_dist[i, j]  # shape [N*(N-1)/2]\n",
    "\n",
    "        # Compute stats\n",
    "        mean_cs = cs.mean().item()\n",
    "        std_cs = cs.std(unbiased=True).item()\n",
    "        mean_td = td.mean().item()\n",
    "        std_td = td.std(unbiased=True).item()\n",
    "\n",
    "        # Compute correlation\n",
    "        pearson_corr, pearson_p = pearsonr(cs.cpu().numpy(), td.cpu().numpy())\n",
    "        spearman_corr, spearman_p = spearmanr(cs.cpu().numpy(), td.cpu().numpy())\n",
    "\n",
    "        print(f\"Depth {d}\")\n",
    "        print(f\"  Mean cosine similarity: {mean_cs:.4f} (σ={std_cs:.4f})\")\n",
    "        print(f\"  Mean Tanimoto distance: {mean_td:.4f} (σ={std_td:.4f})\")\n",
    "        print(f\"  Pearson correlation: {pearson_corr:.4f} (p={pearson_p:.2e})\")\n",
    "        print(f\"  Spearman correlation: {spearman_corr:.4f} (p={spearman_p:.2e})\")\n",
    "        print()\n",
    "\n",
    "        # Plot: histogram comparison\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "        axes[0].hist(cs.cpu().numpy(), bins=50, density=True, alpha=0.75, color='blue')\n",
    "        axes[0].set_xlabel(\"Cosine similarity\")\n",
    "        axes[0].set_ylabel(\"Density\")\n",
    "        axes[0].set_title(f\"Cosine Similarities\\nμ={mean_cs:.3f}, σ={std_cs:.3f}\")\n",
    "\n",
    "        axes[1].hist(td.cpu().numpy(), bins=50, density=True, alpha=0.75, color='red')\n",
    "        axes[1].set_xlabel(\"Tanimoto distance\")\n",
    "        axes[1].set_ylabel(\"Density\")\n",
    "        axes[1].set_title(f\"Tanimoto Distances\\nμ={mean_td:.3f}, σ={std_td:.3f}\")\n",
    "\n",
    "        fig.suptitle(f\"{ds_config.name} - Depth {d}\", fontsize=14, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Plot: scatter plot showing correlation\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.scatter(cs.cpu().numpy(), td.cpu().numpy(), alpha=0.3, s=10)\n",
    "        plt.xlabel(\"Cosine similarity\")\n",
    "        plt.ylabel(\"Tanimoto distance\")\n",
    "        plt.title(f\"{ds_config.name} - Depth {d}\\nPearson r={pearson_corr:.4f}, Spearman ρ={spearman_corr:.4f}\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ],
   "id": "96781c933549136c",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
