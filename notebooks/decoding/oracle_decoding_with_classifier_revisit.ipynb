{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "import itertools\n",
    "import math\n",
    "from collections import OrderedDict, Counter\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Optional, Sequence, Tuple, List\n",
    "from typing import Iterable\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import torch\n",
    "from pytorch_lightning import seed_everything\n",
    "from rdkit import Chem\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Data\n",
    "from torchhd import HRRTensor\n",
    "\n",
    "from src.datasets.zinc_pairs import pyg_to_nx\n",
    "from src.datasets.zinc_smiles_generation import ZincSmiles\n",
    "from src.encoding.configs_and_constants import DatasetConfig, Features, FeatureConfig, IndexRange\n",
    "from src.encoding.feature_encoders import CombinatoricIntegerEncoder\n",
    "from src.encoding.graph_encoders import HyperNet, load_or_create_hypernet\n",
    "from src.encoding.oracles import Oracle\n",
    "from src.encoding.the_types import VSAModel\n",
    "from src.utils.utils import GLOBAL_MODEL_PATH\n",
    "\n",
    "\"\"\"\n",
    "Features\n",
    "    Atom types size: 9\n",
    "    Atom types: ['Br', 'C', 'Cl', 'F', 'I', 'N', 'O', 'P', 'S']\n",
    "    Degrees size: 5, encoded with 0 index:\n",
    "    Degrees: {1, 2, 3, 4, 5}\n",
    "    Formal Charges size: 3\n",
    "    Formal Charges: {0, 1, -1}\n",
    "    Explicit Hs size: 4\n",
    "    Explicit Hs: {0, 1, 2, 3}\n",
    "Encodings:\n",
    "    float(ZINC_SMILE_ATOM_TO_IDX[atom.GetSymbol()]),\n",
    "    float(atom.GetDegree() - 1),  # [1, 2, 3, 4, 5] -> [0, 1, 2, 3, 4]\n",
    "    float(atom.GetFormalCharge() if atom.GetFormalCharge() >= 0 else 2),  # [0, 1, -1] -> [0, 1, 2]\n",
    "    float(atom.GetTotalNumHs()),\n",
    "\"\"\""
   ],
   "id": "cfac9a398aed615f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "seed = 42\n",
    "\n",
    "seed_everything(seed)\n",
    "device = torch.device('cpu')\n",
    "# ----- hypernet config (kept for provenance; not needed in this flow) -----\n",
    "ds_name = \"ZincSmilesHRR7744\"\n",
    "zinc_feature_bins = [9, 6, 3, 4]\n",
    "dataset_config = DatasetConfig(\n",
    "    seed=42,\n",
    "    name=ds_name,\n",
    "    vsa=VSAModel.HRR,\n",
    "    hv_dim=88 * 88,\n",
    "    device=device,\n",
    "    node_feature_configs=OrderedDict(\n",
    "        [\n",
    "            (\n",
    "\n",
    "                Features.ATOM_TYPE,\n",
    "                FeatureConfig(\n",
    "                    count=math.prod(zinc_feature_bins),\n",
    "                    encoder_cls=CombinatoricIntegerEncoder,\n",
    "                    index_range=IndexRange((0, 4)),\n",
    "                    bins=zinc_feature_bins,\n",
    "                ),\n",
    "            ),\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"Loading/creating hypernet â€¦\")\n",
    "hypernet: HyperNet = (\n",
    "    load_or_create_hypernet(path=GLOBAL_MODEL_PATH, cfg=dataset_config).to(device=device)\n",
    ")\n",
    "print(\"Hypernet ready.\")\n",
    "assert not hypernet.use_edge_features()\n",
    "assert not hypernet.use_graph_features()\n",
    "\n"
   ],
   "id": "9537477ab1306191",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "from src.utils import visualisations\n",
    "from src.encoding.decoder import greedy_oracle_decoder\n",
    "from pathlib import Path\n",
    "from src.encoding.oracles import MLPClassifier, Oracle\n",
    "from pprint import pprint\n",
    "from src.utils.utils import DataTransformer\n",
    "from torchhd import HRRTensor\n",
    "\n",
    "\n",
    "# Real Oracle\n",
    "def is_induced_subgraph_feature_aware(G_small: nx.Graph, G_big: nx.Graph) -> bool:\n",
    "    \"\"\"NetworkX VF2: is `G_small` an induced, label-preserving subgraph of `G_big`?\"\"\"\n",
    "    nm = lambda a, b: a[\"feat\"] == b[\"feat\"]\n",
    "    GM = nx.algorithms.isomorphism.GraphMatcher(G_big, G_small, node_match=nm)\n",
    "    return GM.subgraph_is_isomorphic()\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "zinc_smiles = ZincSmiles(split=\"valid\")[:batch_size]\n",
    "dataloader = DataLoader(dataset=zinc_smiles, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Classifier\n",
    "chkpt = torch.load(Path(\"/Users/akaveh/projects/kit/graph_hdc/_models/mlp_stratified_base_laynorm_2nd_try.pt\"), map_location=\"cpu\",\n",
    "                   weights_only=False)\n",
    "\n",
    "cfg = chkpt[\"config\"]\n",
    "# print(f\"Classifier's best metric (AUC): {chkpt['best_metric']}\")\n",
    "print(f\"Classifier's cfg\")\n",
    "pprint(cfg, indent=4)\n",
    "\n",
    "classifier = MLPClassifier(\n",
    "    hv_dim=cfg.get(\"hv_dim\"),\n",
    "    hidden_dims=cfg.get(\"hidden_dims\"),\n",
    "    use_layer_norm=cfg.get(\"use_layer_norm\"),\n",
    "    use_batch_norm=cfg.get(\"use_batch_norm\")).to(device).eval()\n",
    "classifier.load_state_dict(chkpt[\"model_state\"], strict=True)\n",
    "oracle = Oracle(model=classifier)\n",
    "oracle.encoder = hypernet\n",
    "\n",
    "\n",
    "\n",
    "y = []\n",
    "for batch in dataloader:\n",
    "    # Encode the whole graph in one HV\n",
    "    encoded_data = hypernet.forward(batch)\n",
    "    node_term = encoded_data[\"node_terms\"]\n",
    "    graph_term = encoded_data[\"graph_embedding\"]\n",
    "\n",
    "    graph_terms_hd = graph_term.as_subclass(HRRTensor)\n",
    "\n",
    "    ground_truth_counters = {}\n",
    "    datas = batch.to_data_list()\n",
    "    for j, g in enumerate(range(batch_size)):\n",
    "        print(\"================================================\")\n",
    "        full_graph_nx = DataTransformer.pyg_to_nx(data=datas[g])\n",
    "        print(f\"[{j}] Original Graph\")\n",
    "        visualisations.draw_nx_with_atom_colorings(full_graph_nx)\n",
    "        plt.show()\n",
    "        mol_full, _ = DataTransformer.nx_to_mol(full_graph_nx)\n",
    "        display(mol_full)\n",
    "\n",
    "        print(f\"Num Nodes {datas[g].num_nodes}\")\n",
    "        print(f\"Num Edges {int(datas[g].num_edges / 2)}\")\n",
    "        node_multiset = DataTransformer.get_node_counter_from_batch(batch=g, data=batch)\n",
    "        print(f\"Multiset Nodes {node_multiset.total()}\")\n",
    "        nx_GS: list[nx.Graph] = greedy_oracle_decoder(node_multiset=node_multiset, oracle=oracle, full_g_h=graph_terms_hd[g],\n",
    "                                                beam_size=32, oracle_threshold=0.643)\n",
    "        print(len(nx_GS))\n",
    "        print(nx_GS)\n",
    "        nx_GS = list(filter(None, nx_GS))\n",
    "        for i, g in enumerate(nx_GS):\n",
    "            print(f\"Graph Nr: {i}\")\n",
    "            visualisations.draw_nx_with_atom_colorings(g)\n",
    "            plt.show()\n",
    "\n",
    "            mol, _ = DataTransformer.nx_to_mol(g)\n",
    "            display(mol)\n",
    "            print(f\"Num Atoms {mol.GetNumAtoms()}\")\n",
    "            print(f\"Num Bonds {mol.GetNumBonds()}\")\n",
    "\n",
    "            is_induced = is_induced_subgraph_feature_aware(g, full_graph_nx)\n",
    "            print(\"Is Induced subgraph: \", is_induced)\n",
    "            y.append(int(is_induced))\n",
    "\n",
    "print(f\"Accuracy: {sum(y) / len(y)}\")"
   ],
   "id": "a6a00a65ee194c73",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "4c45078077c06252",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
