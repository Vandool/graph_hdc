{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T13:29:43.033159Z",
     "start_time": "2025-07-08T13:29:39.670645Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Conditional Graph Neural Network Implementation for Graph Link Prediction\n",
    "\n",
    "This module implements a conditional graph neural network architecture based on the message passing\n",
    "paradigm, specifically designed for graph link prediction tasks. The implementation uses PyTorch\n",
    "Geometric and PyTorch Lightning frameworks.\n",
    "\n",
    "The key components of this implementation are:\n",
    "\n",
    "1. FilmConditionalLinear: A conditional linear layer using Feature-wise Linear Modulation (FiLM)\n",
    "   that allows neural network behavior to be conditioned on external inputs.\n",
    "\n",
    "2. ConditionalGraphAttention: A graph attention layer that extends PyTorch Geometric's MessagePassing\n",
    "   class, incorporating attention mechanisms and conditional processing via FiLM.\n",
    "\n",
    "3. ConditionalGIN: A Graph Isomorphism Network that uses the conditional graph attention mechanism\n",
    "   for message passing and is trained to predict whether edges should exist in the graph.\n",
    "\n",
    "The module demonstrates how to:\n",
    "- Create conditional neural network layers with FiLM\n",
    "- Implement custom message passing mechanisms with attention\n",
    "- Apply graph neural networks to link prediction tasks\n",
    "- Generate and visualize mock graph data for testing\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import networkx as nx\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.nn.conv import GINConv\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.utils import from_networkx\n",
    "\n",
    "\n",
    "class FilmConditionalLinear(nn.Module):\n",
    "    \"\"\"\n",
    "    This is a conditional variant of the default ``Linear`` layer using the FiLM conditioning mechanism.\n",
    "\n",
    "    As a conditional layer, this layer requires 2 different input tensors. The first is the actual input\n",
    "    tensor to be transformed into the output tensor and the second is the condition vector that should\n",
    "    modify the behavior of the linear layer. The implementation follows the Feature-wise Linear Modulation\n",
    "    (FiLM) approach, which applies an affine transformation (scale and shift) to the output of a linear\n",
    "    layer based on the conditioning vector.\n",
    "\n",
    "    :param in_features: Number of input features\n",
    "    :param out_features: Number of output features\n",
    "    :param condition_features: Number of features in the conditioning vector\n",
    "    :param film_units: List of hidden unit sizes for the FiLM network\n",
    "    :param film_use_norm: Whether to use batch normalization in the FiLM network\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 in_features: int,\n",
    "                 out_features: int,\n",
    "                 condition_features: int,\n",
    "                 film_units: list[int] = [128, ],\n",
    "                 film_use_norm: bool = True,\n",
    "                 **kwargs,\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        Initialize the FiLM conditional linear layer.\n",
    "\n",
    "        :param in_features: Number of input features\n",
    "        :param out_features: Number of output features\n",
    "        :param condition_features: Number of features in the conditioning vector\n",
    "        :param film_units: List of hidden unit sizes for the FiLM network\n",
    "        :param film_use_norm: Whether to use batch normalization in the FiLM network\n",
    "        :param kwargs: Additional keyword arguments to pass to the parent class\n",
    "        \"\"\"\n",
    "        nn.Module.__init__(self, **kwargs)\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.condition_features = condition_features\n",
    "        self.film_units = film_units\n",
    "        self.film_use_norm = film_use_norm\n",
    "        # The final activation we actually want to be Tanh because the output values should\n",
    "        # be in the range of [-1, 1], both for the bias as well as the multiplicative factor.\n",
    "        # TODO: Make this configurable.\n",
    "        self.lay_final_activation = nn.Tanh()\n",
    "\n",
    "        ## -- Main Linear Layer --\n",
    "        # Ultimately, the FiLM layer is just a variation of a linear layer where the output\n",
    "        # is additionally modified by the activation. So what we define here is the core\n",
    "        # linear layer itself.\n",
    "        self.lay_linear = nn.Linear(\n",
    "            in_features=in_features,\n",
    "            out_features=out_features,\n",
    "        )\n",
    "        self.dim = out_features\n",
    "\n",
    "        ## -- FiLM Layers --\n",
    "        # These are the layers that will be used to create the FiLM activation modifier tensors.\n",
    "        # They take as the input the condition vector and transform that into the additive and\n",
    "        # multiplicative modifiers which than perform the affine transformation on the output\n",
    "        # of the actual linear layer.\n",
    "        # This can even be a multi-layer perceptron by itself, depending on how difficult the\n",
    "        # condition function is to learn.\n",
    "        self.film_layers = nn.ModuleList()\n",
    "        prev_features = condition_features\n",
    "        for num_features in film_units:\n",
    "            if self.film_use_norm:\n",
    "                lay = nn.Sequential(\n",
    "                    nn.Linear(\n",
    "                        in_features=prev_features,\n",
    "                        out_features=num_features\n",
    "                    ),\n",
    "                    nn.BatchNorm1d(num_features),\n",
    "                    nn.ReLU(),\n",
    "                )\n",
    "            else:\n",
    "                lay = nn.Sequential(\n",
    "                    nn.Linear(\n",
    "                        in_features=prev_features,\n",
    "                        out_features=num_features\n",
    "                    ),\n",
    "                    nn.ReLU(),\n",
    "                )\n",
    "\n",
    "            self.film_layers.append(lay)\n",
    "            prev_features = num_features\n",
    "\n",
    "        # Finally, at the end of this MLP we need the final layer to be one that outputs a\n",
    "        # vector of the size that is twice the size of the output of the core linear layer.\n",
    "        # From this output we need to derive the additive and the multiplicative modifier\n",
    "        # and we do this by using the first half of the output as the multiplicative\n",
    "        # modifier and the second half as the additive modifier.\n",
    "        self.film_layers.append(nn.Linear(\n",
    "            in_features=prev_features,\n",
    "            out_features=self.dim * 2,\n",
    "        ))\n",
    "\n",
    "    def forward(self,\n",
    "                input: torch.Tensor,\n",
    "                condition: torch.Tensor\n",
    "                ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the FiLM conditional linear layer.\n",
    "\n",
    "        The forward method applies the core linear transformation to the input tensor,\n",
    "        then modifies the result based on the condition tensor through a FiLM (Feature-wise\n",
    "        Linear Modulation) mechanism, which performs an affine transformation with parameters\n",
    "        derived from the condition.\n",
    "\n",
    "        :param input: Input tensor of shape (batch_size, in_features)\n",
    "        :param condition: Condition tensor of shape (batch_size, condition_features)\n",
    "\n",
    "        :returns: Output tensor of shape (batch_size, out_features)\n",
    "        \"\"\"\n",
    "\n",
    "        ## -- getting the modifier from the condition --\n",
    "        # We need the film layers to create the activation modifier tensor.\n",
    "        # This actually may or may not be a multi layer perceptron.\n",
    "        modifier = condition\n",
    "        for lay in self.film_layers:\n",
    "            modifier = lay(modifier)\n",
    "\n",
    "        modifier = 2 * self.lay_final_activation(modifier)\n",
    "\n",
    "        # -- getting the output from the linear layer --\n",
    "        output = self.lay_linear(input)\n",
    "\n",
    "        # -- applying the modifier to the output --\n",
    "        # And then finally we split the modifier vector into the two equally sized distinct vectors where one of them\n",
    "        # is the multiplicative modification and the other is the additive modification to the output activation.\n",
    "        factor = modifier[:, :self.dim]\n",
    "        bias = modifier[:, self.dim:]\n",
    "        output = (factor * output) + bias\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class ConditionalGraphAttention(MessagePassing):\n",
    "    \"\"\"\n",
    "    A conditional graph attention layer that extends PyTorch Geometric's MessagePassing base class.\n",
    "\n",
    "    This layer implements a message passing mechanism where attention coefficients are computed\n",
    "    for each edge based on the features of the connected nodes and edge attributes, modified by\n",
    "    a condition vector. The attention mechanism helps the network focus on the most relevant\n",
    "    parts of the graph structure for the given task and condition.\n",
    "\n",
    "    :param in_dim: Dimension of input node features\n",
    "    :param out_dim: Dimension of output node features\n",
    "    :param edge_dim: Dimension of edge features\n",
    "    :param cond_dim: Dimension of the condition vector\n",
    "    :param hidden_dim: Dimension of hidden layers\n",
    "    :param eps: Epsilon value for residual connections\n",
    "    :param film_units: List of hidden unit sizes for the FiLM networks\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_dim: int,\n",
    "                 out_dim: int,\n",
    "                 edge_dim: int,\n",
    "                 cond_dim: int,\n",
    "                 hidden_dim: int = 64,\n",
    "                 eps: float = 0.1,\n",
    "                 film_units: list[int] = [],\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize the conditional graph attention layer.\n",
    "\n",
    "        :param in_dim: Dimension of input node features\n",
    "        :param out_dim: Dimension of output node features\n",
    "        :param edge_dim: Dimension of edge features\n",
    "        :param cond_dim: Dimension of the condition vector\n",
    "        :param hidden_dim: Dimension of hidden layers\n",
    "        :param eps: Epsilon value for residual connections\n",
    "        :param film_units: List of hidden unit sizes for the FiLM networks\n",
    "        :param kwargs: Additional keyword arguments to pass to the parent class\n",
    "        \"\"\"\n",
    "        kwargs.setdefault('aggr', 'add')\n",
    "        MessagePassing.__init__(self, **kwargs)\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.edge_dim = edge_dim\n",
    "        self.cond_dim = cond_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.eps = eps\n",
    "        self.film_units = film_units\n",
    "\n",
    "        self._attention_logits = None\n",
    "        self._attention = None\n",
    "\n",
    "        ## -- Initial Embedding Layer --\n",
    "        self.message_dim = (in_dim * 2 + edge_dim)\n",
    "        self.lay_message_lin_1 = FilmConditionalLinear(\n",
    "            in_features=self.message_dim,\n",
    "            out_features=self.hidden_dim,\n",
    "            condition_features=self.cond_dim,\n",
    "            film_units=self.film_units,\n",
    "        )\n",
    "        self.lay_message_bn = nn.BatchNorm1d(self.hidden_dim)\n",
    "        self.lay_message_act = nn.LeakyReLU()\n",
    "        self.lay_message_lin_2 = FilmConditionalLinear(\n",
    "            in_features=self.hidden_dim,\n",
    "            out_features=self.hidden_dim,\n",
    "            condition_features=self.cond_dim,\n",
    "            film_units=self.film_units,\n",
    "        )\n",
    "\n",
    "        # -- Attention Layer --\n",
    "        # This layer will produce the attention coefficients which will then be used in the\n",
    "        # attention-weighted message accumulation step.\n",
    "        self.lay_attention_lin_1 = FilmConditionalLinear(\n",
    "            in_features=self.message_dim,\n",
    "            out_features=self.hidden_dim,\n",
    "            condition_features=self.cond_dim,\n",
    "            film_units=self.film_units,\n",
    "        )\n",
    "        self.lay_attention_bn = nn.BatchNorm1d(self.hidden_dim)\n",
    "        self.lay_attention_act = nn.LeakyReLU()\n",
    "        self.lay_attention_lin_2 = FilmConditionalLinear(\n",
    "            in_features=self.hidden_dim,\n",
    "            out_features=1,  # attention logits\n",
    "            condition_features=self.cond_dim,\n",
    "            film_units=self.film_units,\n",
    "        )\n",
    "\n",
    "        # -- Final Transform Layer --\n",
    "        # In the end we add an additional transformation on the attention weighted aggregation\n",
    "        # of the message to determine the update to the node features.\n",
    "        self.lay_transform_lin_1 = FilmConditionalLinear(\n",
    "            in_features=self.hidden_dim + self.in_dim,\n",
    "            out_features=self.hidden_dim,\n",
    "            condition_features=self.cond_dim,\n",
    "            film_units=self.film_units,\n",
    "        )\n",
    "        self.lay_transform_bn = nn.BatchNorm1d(self.hidden_dim)\n",
    "        self.lay_transform_act = nn.LeakyReLU()\n",
    "        self.lay_transform_lin_2 = FilmConditionalLinear(\n",
    "            in_features=self.hidden_dim,\n",
    "            out_features=self.out_dim,\n",
    "            condition_features=self.cond_dim,\n",
    "            film_units=self.film_units,\n",
    "        )\n",
    "\n",
    "    def message(self,\n",
    "                x_i, x_j,\n",
    "                condition_i, condition_j,\n",
    "                edge_attr,\n",
    "                edge_weights,\n",
    "                ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute the message for each edge in the message passing step.\n",
    "\n",
    "        This method is called for each edge during the propagation step of message passing.\n",
    "        It computes attention coefficients based on the features of connected nodes and the edge,\n",
    "        then uses these coefficients to weight the message being passed.\n",
    "\n",
    "        :param x_i: Features of the target node\n",
    "        :param x_j: Features of the source node\n",
    "        :param condition_i: Condition vector for the target node\n",
    "        :param condition_j: Condition vector for the source node\n",
    "        :param edge_attr: Edge attributes\n",
    "        :param edge_weights: Optional edge weights to further modulate the messages\n",
    "\n",
    "        :returns: The weighted message to be passed along the edge\n",
    "        \"\"\"\n",
    "\n",
    "        message = torch.cat([x_i, x_j, edge_attr], dim=-1)\n",
    "\n",
    "        attention_logits = self.lay_attention_lin_1(message, condition_i)\n",
    "        attention_logits = self.lay_attention_bn(attention_logits)\n",
    "        attention_logits = self.lay_attention_act(attention_logits)\n",
    "        attention_logits = self.lay_attention_lin_2(attention_logits, condition_i)\n",
    "        self._attention_logits = attention_logits\n",
    "        self._attention = F.sigmoid(self._attention_logits)\n",
    "\n",
    "        message_transformed = self.lay_message_lin_1(message, condition_i)\n",
    "        message_transformed = self.lay_message_bn(message_transformed)\n",
    "        message_transformed = self.lay_message_act(message_transformed)\n",
    "        message_transformed = self.lay_message_lin_2(message_transformed, condition_i)\n",
    "\n",
    "        result = self._attention * message_transformed\n",
    "\n",
    "        if edge_weights is not None:\n",
    "            if edge_weights.dim() == 1:\n",
    "                edge_weights = torch.unsqueeze(edge_weights, dim=-1)\n",
    "\n",
    "            result *= edge_weights\n",
    "\n",
    "        return result\n",
    "\n",
    "    def forward(self,\n",
    "                x: torch.Tensor,\n",
    "                condition: torch.Tensor,\n",
    "                edge_attr: torch.Tensor,\n",
    "                edge_index: torch.Tensor,\n",
    "                edge_weights: torch.Tensor = None,\n",
    "                **kwargs) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass of the conditional graph attention layer.\n",
    "\n",
    "        This method implements the full message passing operation, including propagation of messages\n",
    "        along edges and aggregation of these messages at each node. The final node embeddings are\n",
    "        computed by transforming the aggregated messages together with the original node features.\n",
    "\n",
    "        :param x: Input node features\n",
    "        :param condition: Condition vector for all nodes\n",
    "        :param edge_attr: Edge attributes\n",
    "        :param edge_index: Graph connectivity\n",
    "        :param edge_weights: Optional edge weights\n",
    "        :param kwargs: Additional keyword arguments\n",
    "\n",
    "        :returns: A tuple containing the updated node embeddings and attention logits\n",
    "        \"\"\"\n",
    "\n",
    "        self._attention = None\n",
    "        self._attention_logits = None\n",
    "\n",
    "        # node_embedding: (B * V, out)\n",
    "        node_embedding = self.propagate(\n",
    "            edge_index,\n",
    "            x=x,\n",
    "            condition=condition,\n",
    "            edge_attr=edge_attr,\n",
    "            edge_weights=edge_weights,\n",
    "        )\n",
    "\n",
    "        # node_embedding = self.lay_act(node_embedding)\n",
    "        x = self.lay_transform_lin_1(\n",
    "            torch.cat([node_embedding, x], axis=1),\n",
    "            condition\n",
    "        )\n",
    "        x = self.lay_transform_bn(x)\n",
    "        x = self.lay_transform_act(x)\n",
    "        x = self.lay_transform_lin_2(x, condition)\n",
    "\n",
    "        # Residual connection to make the gradient flow more stable.\n",
    "        #node_embedding += self.eps * x\n",
    "        node_embedding = x\n",
    "\n",
    "        return node_embedding, self._attention_logits\n",
    "\n",
    "\n",
    "class ConditionalGIN(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    A conditional Graph Isomorphism Network (GIN) implemented using PyTorch Lightning.\n",
    "\n",
    "    This model performs message passing on graph structured data conditioned on an external\n",
    "    vector. It uses the conditional graph attention mechanism to propagate information through\n",
    "    the graph. The model is designed for link prediction tasks, predicting whether edges should\n",
    "    exist in the graph based on the learned node representations and the condition vector.\n",
    "\n",
    "    :param input_dim: Dimension of input node features\n",
    "    :param edge_dim: Dimension of edge features\n",
    "    :param condition_dim: Dimension of the condition vector\n",
    "    :param cond_units: List of hidden unit sizes for the condition embedding network\n",
    "    :param conv_units: List of hidden unit sizes for the graph convolution layers\n",
    "    :param film_units: List of hidden unit sizes for the FiLM networks in the graph attention layers\n",
    "    :param link_units: List of hidden unit sizes for the link prediction network\n",
    "    :param learning_rate: Learning rate for the optimizer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_dim: int,\n",
    "                 edge_dim: int,\n",
    "                 condition_dim: int,\n",
    "                 cond_units: list[int] = [256, 128],\n",
    "                 conv_units: list[int] = [64, 64, 64],\n",
    "                 film_units: list[int] = [128, ],\n",
    "                 link_units: list[int] = [256, 64, 1],\n",
    "                 learning_rate: float = 0.001,\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        Initialize the conditional GIN model.\n",
    "\n",
    "        :param input_dim: Dimension of input node features\n",
    "        :param edge_dim: Dimension of edge features\n",
    "        :param condition_dim: Dimension of the condition vector\n",
    "        :param cond_units: List of hidden unit sizes for the condition embedding network\n",
    "        :param conv_units: List of hidden unit sizes for the graph convolution layers\n",
    "        :param film_units: List of hidden unit sizes for the FiLM networks in the graph attention layers\n",
    "        :param link_units: List of hidden unit sizes for the link prediction network\n",
    "        :param learning_rate: Learning rate for the optimizer\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.condition_dim = condition_dim\n",
    "        self.conv_units = conv_units\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        ## == LAYER DEFINITIONS ==\n",
    "\n",
    "        ## -- Condition Layers --\n",
    "\n",
    "        # These will be the layers (the mlp) which will be used to create an overall lower-dimensional\n",
    "        # embedding representation of the (very high-dimensional) condition vector. It is then this\n",
    "        # embedding that will be used in the individual FiLM conditioning layers.\n",
    "        self.cond_layers = nn.ModuleList()\n",
    "        prev_units = condition_dim\n",
    "        for units in cond_units:\n",
    "            self.cond_layers.append(\n",
    "                nn.Linear(prev_units, units),\n",
    "            )\n",
    "            prev_units = units\n",
    "\n",
    "        self.cond_embedding_dim = prev_units\n",
    "\n",
    "        ## -- Graph Convolutional Layers --\n",
    "\n",
    "        # These will be the actual convolutional layers that will be used as the message passing\n",
    "        # operations on the given graph.\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        prev_units = input_dim\n",
    "        for units in conv_units:\n",
    "            lay = ConditionalGraphAttention(\n",
    "                in_dim=prev_units,\n",
    "                out_dim=units,\n",
    "                edge_dim=edge_dim,\n",
    "                cond_dim=self.cond_embedding_dim,\n",
    "                film_units=film_units,\n",
    "            )\n",
    "            self.conv_layers.append(lay)\n",
    "            prev_units = units\n",
    "\n",
    "        # -- Edge Prediction Layers --\n",
    "\n",
    "        # Finally, after the message passing and so on, we need to have some kind of network which\n",
    "        # takes the messages (edge dimension) as an input and produces the binary classification\n",
    "        # of whether that edge should exist or not.\n",
    "\n",
    "        self.link_layers = nn.ModuleList()\n",
    "        prev_units = self.conv_units[-1] * 2\n",
    "        for i, units in enumerate(link_units):\n",
    "            if i == len(link_units) - 1:\n",
    "                # The last layer should not have the batch norm and activation.\n",
    "                self.link_layers.append(\n",
    "                    nn.Linear(prev_units, units),\n",
    "                )\n",
    "            else:\n",
    "                self.link_layers.append(nn.Sequential(\n",
    "                    nn.Linear(prev_units, units),\n",
    "                    nn.BatchNorm1d(units),\n",
    "                    nn.ReLU(),\n",
    "                ))\n",
    "            prev_units = units\n",
    "\n",
    "        # For the binary classification task we obviously want a sigmoid activation at the end\n",
    "        self.lay_link_act = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, data: Data):\n",
    "        \"\"\"\n",
    "        Forward pass of the conditional GIN model.\n",
    "\n",
    "        This method processes the input graph data through the condition embedding network,\n",
    "        the graph convolutional layers, and finally the link prediction network to predict\n",
    "        edge existence probabilities.\n",
    "\n",
    "        :param data: PyTorch Geometric Data object containing the graph\n",
    "\n",
    "        :returns: Dictionary containing the edge prediction probabilities\n",
    "        \"\"\"\n",
    "\n",
    "        ## -- embedding the condition --\n",
    "        cond: torch.Tensor = data.cond\n",
    "        for lay in self.cond_layers:\n",
    "            cond = lay(cond)\n",
    "\n",
    "        ## -- message passing --\n",
    "        node_embedding = data.x\n",
    "        for lay_conv in self.conv_layers:\n",
    "            node_embedding, _ = lay_conv(\n",
    "                x=node_embedding,\n",
    "                condition=cond,\n",
    "                edge_attr=data.edge_attr,\n",
    "                edge_index=data.edge_index,\n",
    "                edge_weights=data.edge_weights,\n",
    "            )\n",
    "\n",
    "        ## -- link prediction --\n",
    "        node_embedding_i = node_embedding[data.edge_index[0]]\n",
    "        node_embedding_j = node_embedding[data.edge_index[1]]\n",
    "        edge_embedding = torch.cat([node_embedding_i, node_embedding_j], dim=-1)\n",
    "\n",
    "        for lay_link in self.link_layers:\n",
    "            edge_embedding = lay_link(edge_embedding)\n",
    "\n",
    "        # Sigmoid activation to get the final edge prediction probabilities.\n",
    "        edge_prediction = self.lay_link_act(edge_embedding)\n",
    "\n",
    "        return {\n",
    "            'edge_prediction': edge_prediction,\n",
    "        }\n",
    "\n",
    "    def training_step(self, batch: Data, batch_idx):\n",
    "        \"\"\"\n",
    "        Perform a single training step.\n",
    "\n",
    "        This method is called by PyTorch Lightning during training. It computes the forward pass\n",
    "        and calculates the binary cross-entropy loss between the predicted edge probabilities\n",
    "        and the target edge labels.\n",
    "\n",
    "        :param batch: PyTorch Geometric Data object containing a batch of graphs\n",
    "        :param batch_idx: Index of the current batch\n",
    "\n",
    "        :returns: Loss value for the current training step\n",
    "        \"\"\"\n",
    "\n",
    "        batch.edge_y = batch.edge_y.float()\n",
    "\n",
    "        result: dict = self(batch)\n",
    "        edge_prediction = result['edge_prediction']\n",
    "\n",
    "        edge_target = batch.edge_y.view(-1, 1)\n",
    "        loss = F.binary_cross_entropy(\n",
    "            edge_prediction,\n",
    "            edge_target,\n",
    "            reduction='mean',\n",
    "        )\n",
    "\n",
    "        self.log('loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"\n",
    "        Configure optimizers for the model.\n",
    "\n",
    "        This method is called by PyTorch Lightning to set up the optimizer\n",
    "        for training the model.\n",
    "\n",
    "        :returns: The configured optimizer\n",
    "        \"\"\"\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n"
   ],
   "id": "5df3fa1e130836dc",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T13:30:20.270909Z",
     "start_time": "2025-07-08T13:30:20.253690Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Define parameters for our mock data\n",
    "num_graphs = 5\n",
    "node_feature_dim = 8\n",
    "edge_feature_dim = 4\n",
    "condition_dim = 16\n",
    "\n",
    "# Create a list to store our PyG Data objects\n",
    "data_list = []\n",
    "\n",
    "# Generate mock graphs\n",
    "for i in range(num_graphs):\n",
    "    # Create a random graph using networkx\n",
    "    num_nodes = np.random.randint(10, 20)\n",
    "    edge_prob = np.random.uniform(0.2, 0.4)\n",
    "    G = nx.erdos_renyi_graph(n=num_nodes, p=edge_prob, directed=False)\n",
    "\n",
    "    # Add condition vector (same for all nodes in the graph)\n",
    "    condition = np.random.randn(condition_dim)\n",
    "\n",
    "    # Add random node features\n",
    "    for node in G.nodes():\n",
    "        G.nodes[node]['x'] = np.random.randn(node_feature_dim)\n",
    "        G.nodes[node]['cond'] = condition\n",
    "\n",
    "    # Add random edge features and targets\n",
    "    for u, v in G.edges():\n",
    "        G[u][v]['edge_attr'] = np.random.randn(edge_feature_dim)\n",
    "        # Edge target (binary value indicating if the edge should exist)\n",
    "        G[u][v]['edge_y'] = np.random.randint(0, 2)\n",
    "        G[u][v]['edge_weights'] = np.random.uniform(0.5, 1.0)\n",
    "\n",
    "    # Convert networkx graph to PyG Data object\n",
    "    data = from_networkx(G)\n",
    "\n",
    "    # Ensure all tensors have the right type\n",
    "    data.x = data.x.float()\n",
    "    data.cond = data.cond.float()\n",
    "    data.edge_attr = data.edge_attr.float()\n",
    "    data.edge_y = data.edge_y.float()\n",
    "    data.edge_weights = data.edge_weights.float()\n",
    "    a = data.edge_weights\n",
    "\n",
    "    data_list.append(data)\n",
    "\n",
    "# Take the first graph for visualization and model testing\n",
    "sample_data = data_list[0]\n",
    "\n",
    "print(f\"Sample graph info:\")\n",
    "print(f\"Number of nodes: {sample_data.x.size(0)}\")\n",
    "print(f\"Number of edges: {sample_data.edge_index.size(1)}\")\n",
    "print(f\"Node feature dimension: {sample_data.x.size(1)}\")\n",
    "print(f\"Edge feature dimension: {sample_data.edge_attr.size(1)}\")\n",
    "print(f\"Condition dimension: {sample_data.cond.size(1)}\")\n",
    "print(f\"Num Conditions: {sample_data.cond.size(0)}\")\n",
    "print(f\"edge_y: {sample_data.edge_y}\")\n"
   ],
   "id": "581df39f5c10102f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample graph info:\n",
      "Number of nodes: 16\n",
      "Number of edges: 106\n",
      "Node feature dimension: 8\n",
      "Edge feature dimension: 4\n",
      "Condition dimension: 16\n",
      "Num Conditions: 16\n",
      "edge_y: tensor([1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1.,\n",
      "        0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "        1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0.,\n",
      "        0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0.,\n",
      "        0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1.,\n",
      "        1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1.])\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T13:29:43.133846Z",
     "start_time": "2025-07-08T13:29:43.069665Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "    # Visualize the graph with edge targets as colors\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    G_vis = nx.Graph()\n",
    "    for i in range(sample_data.x.size(0)):\n",
    "        G_vis.add_node(i)\n",
    "\n",
    "    edge_colors = []\n",
    "    for i in range(sample_data.edge_index.size(1)):\n",
    "        src = sample_data.edge_index[0, i].item()\n",
    "        dst = sample_data.edge_index[1, i].item()\n",
    "        target = sample_data.edge_y[i].item()\n",
    "        G_vis.add_edge(src, dst)\n",
    "        edge_colors.append('green' if target > 0.5 else 'red')\n",
    "\n",
    "    pos = nx.spring_layout(G_vis, seed=42)\n",
    "    nx.draw(G_vis, pos, with_labels=True, node_color='skyblue',\n",
    "            node_size=500, edge_color=edge_colors, width=2, alpha=0.7)\n",
    "    plt.title(\"Sample Graph with Edge Targets (green=1, red=0)\")\n",
    "    plt.savefig(\"sample_graph.png\")\n",
    "    plt.close()\n"
   ],
   "id": "4cbd4db89f385a1d",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T13:29:44.022968Z",
     "start_time": "2025-07-08T13:29:43.140855Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Initialize the GIN model\n",
    "model = ConditionalGIN(\n",
    "    input_dim=node_feature_dim,\n",
    "    edge_dim=edge_feature_dim,\n",
    "    condition_dim=condition_dim,\n",
    "    cond_units=[64, 32],  # Simplified for testing\n",
    "    conv_units=[32, 32],  # Simplified for testing\n",
    "    film_units=[32],      # Simplified for testing\n",
    "    link_units=[32, 16, 1]\n",
    ")\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Perform a forward pass\n",
    "with torch.no_grad():\n",
    "    result = model(sample_data)\n",
    "    edge_prediction = result['edge_prediction']\n",
    "\n",
    "# Print the results\n",
    "print(\"\\nModel prediction results:\")\n",
    "print(f\"Edge prediction shape: {edge_prediction.shape}\")\n",
    "print(f\"First 5 edge predictions: {edge_prediction[:10].numpy().flatten()}\")\n",
    "print(f\"First 5 edge targets: {sample_data.edge_y[:10].numpy().flatten()}\")\n",
    "\n",
    "loader = DataLoader([sample_data, sample_data], batch_size=2, shuffle=False)\n",
    "batch = next(iter(loader))\n",
    "batch_cond = batch.cond\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "# Prepare to record losses\n",
    "train_losses = []\n",
    "for i in range(50):\n",
    "    print(f\"\\n=== Iteration {i+1} ===\")\n",
    "\n",
    "    # 1) Evaluation pass\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        result = model(sample_data)\n",
    "        edge_pred = result['edge_prediction']\n",
    "    print(\"  [Eval] pred shape:\", edge_pred.shape)\n",
    "    print(\"  [Eval] first 5 preds:\", edge_pred[:10].cpu().numpy().flatten())\n",
    "    print(\"  [Eval] first 5 targets:\", sample_data.edge_y[:10].cpu().numpy().flatten())\n",
    "\n",
    "    # 2) Training step\n",
    "    model.train()\n",
    "    batch = next(iter(loader))\n",
    "    optimizer.zero_grad()\n",
    "    loss = model.training_step(batch, 0)\n",
    "    train_losses.append(loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(\"  [Train] loss:\", loss.item())\n",
    "\n",
    "# After loop, plot losses\n",
    "plt.figure()\n",
    "plt.plot(train_losses)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss over Iterations')\n",
    "plt.show()"
   ],
   "id": "9b318f7d4ee6e0b0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model prediction results:\n",
      "Edge prediction shape: torch.Size([82, 1])\n",
      "First 5 edge predictions: [0.46632078 0.4663199  0.46626267 0.46631283 0.46632156 0.46626568\n",
      " 0.4662947  0.46633446 0.4663252  0.46629253]\n",
      "First 5 edge targets: [1. 0. 0. 1. 0. 0. 0. 1. 1. 1.]\n",
      "\n",
      "=== Iteration 1 ===\n",
      "  [Eval] pred shape: torch.Size([82, 1])\n",
      "  [Eval] first 5 preds: [0.46632078 0.4663199  0.46626267 0.46631283 0.46632156 0.46626568\n",
      " 0.4662947  0.46633446 0.4663252  0.46629253]\n",
      "  [Eval] first 5 targets: [1. 0. 0. 1. 0. 0. 0. 1. 1. 1.]\n",
      "  [Train] loss: 0.756715714931488\n",
      "\n",
      "=== Iteration 2 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arvandkaveh/Projects/kit/graph_hdc/.pixi/envs/default/lib/python3.13/site-packages/pytorch_lightning/core/module.py:445: You are trying to `self.log()` but the `self.trainer` reference is not registered on the model yet. This is most likely because the model hasn't been passed to the `Trainer`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [Eval] pred shape: torch.Size([82, 1])\n",
      "  [Eval] first 5 preds: [0.46828163 0.46828362 0.46829498 0.46828023 0.46828184 0.4682923\n",
      " 0.46828404 0.46829486 0.46829668 0.46829638]\n",
      "  [Eval] first 5 targets: [1. 0. 0. 1. 0. 0. 0. 1. 1. 1.]\n",
      "  [Train] loss: 0.6666660904884338\n",
      "\n",
      "=== Iteration 3 ===\n",
      "  [Eval] pred shape: torch.Size([82, 1])\n",
      "  [Eval] first 5 preds: [0.47079456 0.47079605 0.4708185  0.47079277 0.47079456 0.47081494\n",
      " 0.47079933 0.47080427 0.4708061  0.47081232]\n",
      "  [Eval] first 5 targets: [1. 0. 0. 1. 0. 0. 0. 1. 1. 1.]\n",
      "  [Train] loss: 0.6185042262077332\n",
      "\n",
      "=== Iteration 4 ===\n",
      "  [Eval] pred shape: torch.Size([82, 1])\n",
      "  [Eval] first 5 preds: [0.47247365 0.4724742  0.47249115 0.47247285 0.47247487 0.47248816\n",
      " 0.4724784  0.4724832  0.47248366 0.472487  ]\n",
      "  [Eval] first 5 targets: [1. 0. 0. 1. 0. 0. 0. 1. 1. 1.]\n",
      "  [Train] loss: 0.5915088057518005\n",
      "\n",
      "=== Iteration 5 ===\n",
      "  [Eval] pred shape: torch.Size([82, 1])\n",
      "  [Eval] first 5 preds: [0.47090513 0.47090462 0.47091106 0.4709032  0.47090703 0.47090894\n",
      " 0.4709028  0.4709147  0.4709133  0.4709112 ]\n",
      "  [Eval] first 5 targets: [1. 0. 0. 1. 0. 0. 0. 1. 1. 1.]\n",
      "  [Train] loss: 0.5705618262290955\n",
      "\n",
      "=== Iteration 6 ===\n",
      "  [Eval] pred shape: torch.Size([82, 1])\n",
      "  [Eval] first 5 preds: [0.46904227 0.46904054 0.4690217  0.46903643 0.46904448 0.46902135\n",
      " 0.46902785 0.46905306 0.46904725 0.46903098]\n",
      "  [Eval] first 5 targets: [1. 0. 0. 1. 0. 0. 0. 1. 1. 1.]\n",
      "  [Train] loss: 0.5569545030593872\n",
      "\n",
      "=== Iteration 7 ===\n",
      "  [Eval] pred shape: torch.Size([82, 1])\n",
      "  [Eval] first 5 preds: [0.46764246 0.46764168 0.46766484 0.46764243 0.46764383 0.46766207\n",
      " 0.4676482  0.46765044 0.4676508  0.4676597 ]\n",
      "  [Eval] first 5 targets: [1. 0. 0. 1. 0. 0. 0. 1. 1. 1.]\n",
      "  [Train] loss: 0.5435715913772583\n",
      "\n",
      "=== Iteration 8 ===\n",
      "  [Eval] pred shape: torch.Size([82, 1])\n",
      "  [Eval] first 5 preds: [0.4666891  0.46668863 0.4667291  0.4666916  0.46669227 0.46672556\n",
      " 0.46670645 0.46669307 0.46669602 0.4667183 ]\n",
      "  [Eval] first 5 targets: [1. 0. 0. 1. 0. 0. 0. 1. 1. 1.]\n",
      "  [Train] loss: 0.5326778888702393\n",
      "\n",
      "=== Iteration 9 ===\n",
      "  [Eval] pred shape: torch.Size([82, 1])\n",
      "  [Eval] first 5 preds: [0.4651162  0.46511698 0.46519834 0.46512318 0.4651241  0.4651925\n",
      " 0.4651515  0.46510312 0.46511057 0.4651532 ]\n",
      "  [Eval] first 5 targets: [1. 0. 0. 1. 0. 0. 0. 1. 1. 1.]\n",
      "  [Train] loss: 0.5207431316375732\n",
      "\n",
      "=== Iteration 10 ===\n",
      "  [Eval] pred shape: torch.Size([82, 1])\n",
      "  [Eval] first 5 preds: [0.46236363 0.4623657  0.46242204 0.46237123 0.46237317 0.4624185\n",
      " 0.46239397 0.46234742 0.46235743 0.46238577]\n",
      "  [Eval] first 5 targets: [1. 0. 0. 1. 0. 0. 0. 1. 1. 1.]\n",
      "  [Train] loss: 0.5085535049438477\n",
      "\n",
      "=== Iteration 11 ===\n",
      "  [Eval] pred shape: torch.Size([82, 1])\n",
      "  [Eval] first 5 preds: [0.45979002 0.4597879  0.4598611  0.45978862 0.45980132 0.45985523\n",
      " 0.45981055 0.45979103 0.45978802 0.45981997]\n",
      "  [Eval] first 5 targets: [1. 0. 0. 1. 0. 0. 0. 1. 1. 1.]\n",
      "  [Train] loss: 0.4984799921512604\n",
      "\n",
      "=== Iteration 12 ===\n",
      "  [Eval] pred shape: torch.Size([82, 1])\n",
      "  [Eval] first 5 preds: [0.4573929  0.4573914  0.45737395 0.4573909  0.45740452 0.45736867\n",
      " 0.45739466 0.45742086 0.45742399 0.45742354]\n",
      "  [Eval] first 5 targets: [1. 0. 0. 1. 0. 0. 0. 1. 1. 1.]\n",
      "  [Train] loss: 0.48955583572387695\n",
      "\n",
      "=== Iteration 13 ===\n",
      "  [Eval] pred shape: torch.Size([82, 1])\n",
      "  [Eval] first 5 preds: [0.45511457 0.45511994 0.45511907 0.45512712 0.45512712 0.45511556\n",
      " 0.45512924 0.45514804 0.4551608  0.45517296]\n",
      "  [Eval] first 5 targets: [1. 0. 0. 1. 0. 0. 0. 1. 1. 1.]\n",
      "  [Train] loss: 0.4810927212238312\n",
      "\n",
      "=== Iteration 14 ===\n",
      "  [Eval] pred shape: torch.Size([82, 1])\n",
      "  [Eval] first 5 preds: [0.4523156  0.4523219  0.45236716 0.45233107 0.45233077 0.45236278\n",
      " 0.4523801  0.4523579  0.4523674  0.4524349 ]\n",
      "  [Eval] first 5 targets: [1. 0. 0. 1. 0. 0. 0. 1. 1. 1.]\n",
      "  [Train] loss: 0.47398898005485535\n",
      "\n",
      "=== Iteration 15 ===\n",
      "  [Eval] pred shape: torch.Size([82, 1])\n",
      "  [Eval] first 5 preds: [0.4492112  0.44923282 0.44931948 0.44926608 0.44923037 0.44931585\n",
      " 0.4493539  0.44917384 0.4492524  0.4493811 ]\n",
      "  [Eval] first 5 targets: [1. 0. 0. 1. 0. 0. 0. 1. 1. 1.]\n",
      "  [Train] loss: 0.46631553769111633\n",
      "\n",
      "=== Iteration 16 ===\n",
      "  [Eval] pred shape: torch.Size([82, 1])\n",
      "  [Eval] first 5 preds: [0.44626227 0.44628203 0.44638747 0.44632387 0.44627628 0.44638115\n",
      " 0.4464447  0.4461959  0.4462805  0.44645134]\n",
      "  [Eval] first 5 targets: [1. 0. 0. 1. 0. 0. 0. 1. 1. 1.]\n",
      "  [Train] loss: 0.4580269753932953\n",
      "\n",
      "=== Iteration 17 ===\n",
      "  [Eval] pred shape: torch.Size([82, 1])\n",
      "  [Eval] first 5 preds: [0.44332528 0.44335085 0.4435307  0.44338226 0.44334152 0.44352648\n",
      " 0.4435672  0.4432417  0.44334397 0.4436108 ]\n",
      "  [Eval] first 5 targets: [1. 0. 0. 1. 0. 0. 0. 1. 1. 1.]\n",
      "  [Train] loss: 0.4509042799472809\n",
      "\n",
      "=== Iteration 18 ===\n",
      "  [Eval] pred shape: torch.Size([82, 1])\n",
      "  [Eval] first 5 preds: [0.44045278 0.4404789  0.44067782 0.44051397 0.44047332 0.44066945\n",
      " 0.44073522 0.44036832 0.44046393 0.44074124]\n",
      "  [Eval] first 5 targets: [1. 0. 0. 1. 0. 0. 0. 1. 1. 1.]\n",
      "  [Train] loss: 0.4439304769039154\n",
      "\n",
      "=== Iteration 19 ===\n",
      "  [Eval] pred shape: torch.Size([82, 1])\n",
      "  [Eval] first 5 preds: [0.43742478 0.4374638  0.43767405 0.43750775 0.4374556  0.4376633\n",
      " 0.43772507 0.4373361  0.43744218 0.4377234 ]\n",
      "  [Eval] first 5 targets: [1. 0. 0. 1. 0. 0. 0. 1. 1. 1.]\n",
      "  [Train] loss: 0.4368673264980316\n",
      "\n",
      "=== Iteration 20 ===\n",
      "  [Eval] pred shape: torch.Size([82, 1])\n",
      "  [Eval] first 5 preds: [0.4342773  0.43433586 0.4345838  0.4343902  0.43431237 0.434566\n",
      " 0.43460876 0.43416348 0.43430346 0.43460202]\n",
      "  [Eval] first 5 targets: [1. 0. 0. 1. 0. 0. 0. 1. 1. 1.]\n",
      "  [Train] loss: 0.4293121099472046\n",
      "\n",
      "=== Iteration 21 ===\n",
      "  [Eval] pred shape: torch.Size([82, 1])\n",
      "  [Eval] first 5 preds: [0.43151817 0.43155974 0.43193957 0.4316168  0.4315489  0.4319141\n",
      " 0.4319355  0.43136972 0.43150645 0.43193644]\n",
      "  [Eval] first 5 targets: [1. 0. 0. 1. 0. 0. 0. 1. 1. 1.]\n",
      "  [Train] loss: 0.4216596186161041\n",
      "\n",
      "=== Iteration 22 ===\n",
      "  [Eval] pred shape: torch.Size([82, 1])\n",
      "  [Eval] first 5 preds: [0.42876053 0.4288072  0.42943108 0.42887807 0.42879015 0.42939317\n",
      " 0.42923582 0.4285826  0.4287387  0.4293093 ]\n",
      "  [Eval] first 5 targets: [1. 0. 0. 1. 0. 0. 0. 1. 1. 1.]\n",
      "  [Train] loss: 0.41478654742240906\n",
      "\n",
      "=== Iteration 23 ===\n",
      "  [Eval] pred shape: torch.Size([82, 1])\n",
      "  [Eval] first 5 preds: [0.42535874 0.42540565 0.42674664 0.42550907 0.42538452 0.42653805\n",
      " 0.4259755  0.42514786 0.42535767 0.42606527]\n",
      "  [Eval] first 5 targets: [1. 0. 0. 1. 0. 0. 0. 1. 1. 1.]\n",
      "  [Train] loss: 0.40799379348754883\n",
      "\n",
      "=== Iteration 24 ===\n",
      "  [Eval] pred shape: torch.Size([82, 1])\n",
      "  [Eval] first 5 preds: [0.4221176  0.42214423 0.42451623 0.4222815  0.42213252 0.42431644\n",
      " 0.42290345 0.42186448 0.42210326 0.42307657]\n",
      "  [Eval] first 5 targets: [1. 0. 0. 1. 0. 0. 0. 1. 1. 1.]\n",
      "  [Train] loss: 0.4016770124435425\n",
      "\n",
      "=== Iteration 25 ===\n",
      "  [Eval] pred shape: torch.Size([82, 1])\n",
      "  [Eval] first 5 preds: [0.41814786 0.41817778 0.42099413 0.41833043 0.41815978 0.42075098\n",
      " 0.41896698 0.41790214 0.41816556 0.41926354]\n",
      "  [Eval] first 5 targets: [1. 0. 0. 1. 0. 0. 0. 1. 1. 1.]\n",
      "  [Train] loss: 0.39597803354263306\n",
      "\n",
      "=== Iteration 26 ===\n",
      "  [Eval] pred shape: torch.Size([82, 1])\n",
      "  [Eval] first 5 preds: [0.41476804 0.41479042 0.41812292 0.41497776 0.41476563 0.4178135\n",
      " 0.4156821  0.41453063 0.41481555 0.416015  ]\n",
      "  [Eval] first 5 targets: [1. 0. 0. 1. 0. 0. 0. 1. 1. 1.]\n",
      "  [Train] loss: 0.3900589048862457\n",
      "\n",
      "=== Iteration 27 ===\n",
      "  [Eval] pred shape: torch.Size([82, 1])\n",
      "  [Eval] first 5 preds: [0.41259226 0.41259608 0.41680065 0.41283223 0.41257155 0.4163904\n",
      " 0.41364002 0.41232133 0.4126434  0.41409245]\n",
      "  [Eval] first 5 targets: [1. 0. 0. 1. 0. 0. 0. 1. 1. 1.]\n",
      "  [Train] loss: 0.38442593812942505\n",
      "\n",
      "=== Iteration 28 ===\n",
      "  [Eval] pred shape: torch.Size([82, 1])\n",
      "  [Eval] first 5 preds: [0.412176   0.41215724 0.4170607  0.41244587 0.41212937 0.41673627\n",
      " 0.4134878  0.41185394 0.41222873 0.4138044 ]\n",
      "  [Eval] first 5 targets: [1. 0. 0. 1. 0. 0. 0. 1. 1. 1.]\n",
      "  [Train] loss: 0.37856581807136536\n",
      "\n",
      "=== Iteration 29 ===\n",
      "  [Eval] pred shape: torch.Size([82, 1])\n",
      "  [Eval] first 5 preds: [0.41130325 0.41121256 0.4169673  0.4114314  0.41126305 0.41649675\n",
      " 0.41314209 0.41104445 0.41128576 0.413541  ]\n",
      "  [Eval] first 5 targets: [1. 0. 0. 1. 0. 0. 0. 1. 1. 1.]\n",
      "  [Train] loss: 0.3727501332759857\n",
      "\n",
      "=== Iteration 30 ===\n",
      "  [Eval] pred shape: torch.Size([82, 1])\n",
      "  [Eval] first 5 preds: [0.4100246  0.40990692 0.41668287 0.41019025 0.4099646  0.4160907\n",
      " 0.4124983  0.4097687  0.41001067 0.41297105]\n",
      "  [Eval] first 5 targets: [1. 0. 0. 1. 0. 0. 0. 1. 1. 1.]\n",
      "  [Train] loss: 0.36764463782310486\n",
      "\n",
      "=== Iteration 31 ===\n",
      "  [Eval] pred shape: torch.Size([82, 1])\n",
      "  [Eval] first 5 preds: [0.4098444  0.40970543 0.4169113  0.41007552 0.4097642  0.41617247\n",
      " 0.41257623 0.40953916 0.40986112 0.41288748]\n",
      "  [Eval] first 5 targets: [1. 0. 0. 1. 0. 0. 0. 1. 1. 1.]\n",
      "  [Train] loss: 0.36282697319984436\n",
      "\n",
      "=== Iteration 32 ===\n",
      "  [Eval] pred shape: torch.Size([82, 1])\n",
      "  [Eval] first 5 preds: [0.40957445 0.40947264 0.415422   0.4099371  0.4095341  0.41471952\n",
      " 0.410811   0.40919396 0.40960428 0.41119435]\n",
      "  [Eval] first 5 targets: [1. 0. 0. 1. 0. 0. 0. 1. 1. 1.]\n",
      "  [Train] loss: 0.3574451208114624\n",
      "\n",
      "=== Iteration 33 ===\n",
      "  [Eval] pred shape: torch.Size([82, 1])\n",
      "  [Eval] first 5 preds: [0.4097311  0.4095975  0.41247833 0.4101439  0.4096625  0.41185144\n",
      " 0.4103023  0.40932995 0.40975863 0.41100463]\n",
      "  [Eval] first 5 targets: [1. 0. 0. 1. 0. 0. 0. 1. 1. 1.]\n",
      "  [Train] loss: 0.35224449634552\n",
      "\n",
      "=== Iteration 34 ===\n",
      "  [Eval] pred shape: torch.Size([82, 1])\n",
      "  [Eval] first 5 preds: [0.4114662  0.4112668  0.41112727 0.4113796  0.41136637 0.4110885\n",
      " 0.41141194 0.41096792 0.4114732  0.41222087]\n",
      "  [Eval] first 5 targets: [1. 0. 0. 1. 0. 0. 0. 1. 1. 1.]\n",
      "  [Train] loss: 0.3473328649997711\n",
      "\n",
      "=== Iteration 35 ===\n",
      "  [Eval] pred shape: torch.Size([82, 1])\n",
      "  [Eval] first 5 preds: [0.41279253 0.41251057 0.4120002  0.41270363 0.4126919  0.41201282\n",
      " 0.41259527 0.41281995 0.41292062 0.41335458]\n",
      "  [Eval] first 5 targets: [1. 0. 0. 1. 0. 0. 0. 1. 1. 1.]\n",
      "  [Train] loss: 0.3427431583404541\n",
      "\n",
      "=== Iteration 36 ===\n",
      "  [Eval] pred shape: torch.Size([82, 1])\n",
      "  [Eval] first 5 preds: [0.41407767 0.41367167 0.4128747  0.41396606 0.4139803  0.41270155\n",
      " 0.41354632 0.4148109  0.41446504 0.41431755]\n",
      "  [Eval] first 5 targets: [1. 0. 0. 1. 0. 0. 0. 1. 1. 1.]\n",
      "  [Train] loss: 0.33810099959373474\n",
      "\n",
      "=== Iteration 37 ===\n",
      "  [Eval] pred shape: torch.Size([82, 1])\n",
      "  [Eval] first 5 preds: [0.41613126 0.41581517 0.41419038 0.4157742  0.41614544 0.41386193\n",
      " 0.4147073  0.4171779  0.41682056 0.41579264]\n",
      "  [Eval] first 5 targets: [1. 0. 0. 1. 0. 0. 0. 1. 1. 1.]\n",
      "  [Train] loss: 0.3336155116558075\n",
      "\n",
      "=== Iteration 38 ===\n",
      "  [Eval] pred shape: torch.Size([82, 1])\n",
      "  [Eval] first 5 preds: [0.41753554 0.4176339  0.41449273 0.41702342 0.4179486  0.41396624\n",
      " 0.41541046 0.41896936 0.41866916 0.4166329 ]\n",
      "  [Eval] first 5 targets: [1. 0. 0. 1. 0. 0. 0. 1. 1. 1.]\n",
      "  [Train] loss: 0.3294450342655182\n",
      "\n",
      "=== Iteration 39 ===\n",
      "  [Eval] pred shape: torch.Size([82, 1])\n",
      "  [Eval] first 5 preds: [0.41460186 0.41497836 0.41085914 0.41430932 0.41535485 0.4101376\n",
      " 0.4124535  0.41652805 0.41617623 0.41379064]\n",
      "  [Eval] first 5 targets: [1. 0. 0. 1. 0. 0. 0. 1. 1. 1.]\n",
      "  [Train] loss: 0.32540857791900635\n",
      "\n",
      "=== Iteration 40 ===\n",
      "  [Eval] pred shape: torch.Size([82, 1])\n",
      "  [Eval] first 5 preds: [0.40877408 0.4094172  0.403913   0.40851942 0.41019154 0.4035371\n",
      " 0.40635613 0.410953   0.41069323 0.4079132 ]\n",
      "  [Eval] first 5 targets: [1. 0. 0. 1. 0. 0. 0. 1. 1. 1.]\n",
      "  [Train] loss: 0.3213723599910736\n",
      "\n",
      "=== Iteration 41 ===\n",
      "  [Eval] pred shape: torch.Size([82, 1])\n",
      "  [Eval] first 5 preds: [0.40504766 0.40605    0.39974195 0.40476215 0.40674555 0.399172\n",
      " 0.4013785  0.40729037 0.40730983 0.4031327 ]\n",
      "  [Eval] first 5 targets: [1. 0. 0. 1. 0. 0. 0. 1. 1. 1.]\n",
      "  [Train] loss: 0.3173825442790985\n",
      "\n",
      "=== Iteration 42 ===\n",
      "  [Eval] pred shape: torch.Size([82, 1])\n",
      "  [Eval] first 5 preds: [0.4034827  0.4049883  0.39446777 0.40269372 0.4058283  0.39567488\n",
      " 0.3991443  0.40659335 0.40669987 0.40153602]\n",
      "  [Eval] first 5 targets: [1. 0. 0. 1. 0. 0. 0. 1. 1. 1.]\n",
      "  [Train] loss: 0.3135194182395935\n",
      "\n",
      "=== Iteration 43 ===\n",
      "  [Eval] pred shape: torch.Size([82, 1])\n",
      "  [Eval] first 5 preds: [0.40352565 0.40615687 0.38886034 0.40244117 0.40630668 0.39099535\n",
      " 0.3947411  0.40673003 0.40795138 0.39984658]\n",
      "  [Eval] first 5 targets: [1. 0. 0. 1. 0. 0. 0. 1. 1. 1.]\n",
      "  [Train] loss: 0.30997833609580994\n",
      "\n",
      "=== Iteration 44 ===\n",
      "  [Eval] pred shape: torch.Size([82, 1])\n",
      "  [Eval] first 5 preds: [0.40469906 0.40838757 0.38405204 0.4002171  0.40825197 0.38719255\n",
      " 0.3893524  0.40684387 0.4101572  0.3950387 ]\n",
      "  [Eval] first 5 targets: [1. 0. 0. 1. 0. 0. 0. 1. 1. 1.]\n",
      "  [Train] loss: 0.3063216805458069\n",
      "\n",
      "=== Iteration 45 ===\n",
      "  [Eval] pred shape: torch.Size([82, 1])\n",
      "  [Eval] first 5 preds: [0.40335277 0.40707672 0.3809229  0.39756352 0.40960413 0.38429087\n",
      " 0.38638264 0.40397295 0.40722597 0.3903196 ]\n",
      "  [Eval] first 5 targets: [1. 0. 0. 1. 0. 0. 0. 1. 1. 1.]\n",
      "  [Train] loss: 0.3027600049972534\n",
      "\n",
      "=== Iteration 46 ===\n",
      "  [Eval] pred shape: torch.Size([82, 1])\n",
      "  [Eval] first 5 preds: [0.39840308 0.40413487 0.3724967  0.3930788  0.40676606 0.37637964\n",
      " 0.38082632 0.39990976 0.40358308 0.38418877]\n",
      "  [Eval] first 5 targets: [1. 0. 0. 1. 0. 0. 0. 1. 1. 1.]\n",
      "  [Train] loss: 0.2991221249103546\n",
      "\n",
      "=== Iteration 47 ===\n",
      "  [Eval] pred shape: torch.Size([82, 1])\n",
      "  [Eval] first 5 preds: [0.38648838 0.39005145 0.35488462 0.37966177 0.3916276  0.36036018\n",
      " 0.36679015 0.38843828 0.3899641  0.36897144]\n",
      "  [Eval] first 5 targets: [1. 0. 0. 1. 0. 0. 0. 1. 1. 1.]\n",
      "  [Train] loss: 0.2954255938529968\n",
      "\n",
      "=== Iteration 48 ===\n",
      "  [Eval] pred shape: torch.Size([82, 1])\n",
      "  [Eval] first 5 preds: [0.38175938 0.38165015 0.34460488 0.37331143 0.38583082 0.35147527\n",
      " 0.3575644  0.38318852 0.3823795  0.36106676]\n",
      "  [Eval] first 5 targets: [1. 0. 0. 1. 0. 0. 0. 1. 1. 1.]\n",
      "  [Train] loss: 0.2918664515018463\n",
      "\n",
      "=== Iteration 49 ===\n",
      "  [Eval] pred shape: torch.Size([82, 1])\n",
      "  [Eval] first 5 preds: [0.36245093 0.35996434 0.31640118 0.35411277 0.36448583 0.32829612\n",
      " 0.33592227 0.3665845  0.3642385  0.3427877 ]\n",
      "  [Eval] first 5 targets: [1. 0. 0. 1. 0. 0. 0. 1. 1. 1.]\n",
      "  [Train] loss: 0.2886205017566681\n",
      "\n",
      "=== Iteration 50 ===\n",
      "  [Eval] pred shape: torch.Size([82, 1])\n",
      "  [Eval] first 5 preds: [0.32044387 0.31972903 0.2832741  0.31506783 0.3256253  0.29058942\n",
      " 0.28940845 0.32084772 0.3274921  0.30673897]\n",
      "  [Eval] first 5 targets: [1. 0. 0. 1. 0. 0. 0. 1. 1. 1.]\n",
      "  [Train] loss: 0.2853285074234009\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 5
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
