{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-03T14:32:34.337807Z",
     "start_time": "2025-06-03T14:32:34.315386Z"
    }
   },
   "source": [
    "from torchhd import VTBTensor\n",
    "\n",
    "from src.encoding.the_types import VSAModel\n",
    "\n",
    "\"\"\"\n",
    "autograd_check.py\n",
    "\n",
    "A small script that verifies whether torchhd.bind, torchhd.bundle, torchhd.multibind,\n",
    "torchhd.multibundle, and scatter_hd all support backpropagation (i.e., produce valid\n",
    "gradients under PyTorch’s autograd). We test this for each TorchHD VSA model (MAP, HRR,\n",
    "BSC/Ternary, etc.) and for each of the four reduction ops.\n",
    "\n",
    "Usage:\n",
    "    python autograd_check.py\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.utils import scatter\n",
    "import torch.optim as optim\n",
    "import torchhd\n",
    "\n",
    "# Import all available hypervector types from torchhd.tensors.*\n",
    "from torchhd.tensors.map import MAPTensor       #  [oai_citation:0‡torchhd.readthedocs.io](https://torchhd.readthedocs.io/en/stable/torchhd.html?utm_source=chatgpt.com)\n",
    "from torchhd.tensors.hrr import HRRTensor         #  [oai_citation:1‡torchhd.readthedocs.io](https://torchhd.readthedocs.io/en/stable/torchhd.html?utm_source=chatgpt.com)\n",
    "from torchhd.tensors.bsbc import BSBCTensor       #  [oai_citation:2‡aidoczh.com](https://www.aidoczh.com/torchhd/_modules/torchhd/tensors/bsbc.html?utm_source=chatgpt.com)\n",
    "from torchhd.tensors.fhrr import FHRRTensor       # (if installed)  [oai_citation:3‡torchhd.readthedocs.io](https://torchhd.readthedocs.io/en/stable/getting_started.html?utm_source=chatgpt.com)\n",
    "\n",
    "import random\n",
    "\n",
    "# 1) gather all torchhd tensor‐classes to test\n",
    "VSA_TYPES = {\n",
    "    \"MAP\": MAPTensor,\n",
    "    \"HRR\": HRRTensor,\n",
    "    \"FHRR\": FHRRTensor,\n",
    "    \"VTB\": VTBTensor,\n",
    "}\n",
    "\n",
    "\n",
    "# 2) helper: wrap torchhd.bind / bundle etc. in a single place\n",
    "OP_FUNCS = {\n",
    "    \"bind\": torchhd.bind,\n",
    "    \"bundle\": torchhd.bundle,\n",
    "}\n",
    "\n",
    "MULTI_FUNCS = {\n",
    "    \"multibind\": torchhd.multibind,\n",
    "    \"multibundle\": torchhd.multibundle,\n",
    "}\n",
    "\n",
    "# 3) Scatter‐HD helper (flattened) as given in the prompt\n",
    "from typing import Literal\n",
    "ReductionOP = Literal[\"bind\", \"bundle\"]\n",
    "\n",
    "def scatter_hd(\n",
    "        src: Tensor,\n",
    "        index: Tensor,\n",
    "        *,\n",
    "        op: ReductionOP,\n",
    "        dim_size: int | None = None,\n",
    ") -> Tensor:\n",
    "    \"\"\"\n",
    "    Scatter-reduce a batch of hypervectors along dim=0 using\n",
    "    either torchhd.bind or torchhd.bundle, with minimal overhead\n",
    "    for MAP, BSC and HRR models.\n",
    "\n",
    "    Args:\n",
    "        src (Tensor): hypervector batch of shape [N, D, ...] where\n",
    "                      N is the “items” dimension to scatter over.\n",
    "        index (LongTensor): shape [N], bucket indices in [0..dim_size).\n",
    "        op (Callable): either torchhd.bind or torchhd.bundle.\n",
    "        dim_size (int, optional): number of output buckets.\n",
    "                                   If None, uses index.max()+1.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: scattered & reduced hypervectors of shape\n",
    "                [dim_size, D, ...], same dtype/device as src.\n",
    "    \"\"\"\n",
    "    # infer output size\n",
    "    if dim_size is None:\n",
    "        dim_size = int(index.max().item()) + 1\n",
    "\n",
    "    # dispatch on type and op\n",
    "    reduce = \"\"\n",
    "    if isinstance(src, MAPTensor):\n",
    "        # MAP bind == elementwise multiply → scatter-mul\n",
    "        # MAP bundle == elementwise sum → use pyg scatter-sum\n",
    "        reduce = \"sum\" if op == \"bundle\" else \"mul\"\n",
    "    elif isinstance(src, HRRTensor) and op == \"bundle\":\n",
    "        # HRR bundle == elementwise sum\n",
    "        reduce = \"sum\"\n",
    "        # HRR bind (circular conv) not supported by pyg\n",
    "    if reduce:\n",
    "        # When the dim_size is bigger than the addressed indexes, the scatter stacks zero vectors to reach the desired\n",
    "        # dimensions, this is not correct in the hyperdimensional algebra. There we need identity vectors, such vectors\n",
    "        # that when bound with a random-hypervector X, the result is X. Therefore we need to add them manually\n",
    "        idx_dim = int(index.max().item()) + 1\n",
    "        result = scatter(src, index, dim=0, dim_size=idx_dim, reduce=reduce)\n",
    "\n",
    "        if (num_identity_vectors := dim_size - idx_dim) == 0:\n",
    "            return result\n",
    "\n",
    "        # TODO: Improve this\n",
    "        vsa = VSAModel.HRR\n",
    "        if VSAModel.MAP.value in repr(type(src)):\n",
    "            vsa = VSAModel.MAP\n",
    "        # elif VSAModel.BSC.value in repr(type(src)):\n",
    "        #     vsa = VSAModel.BSC\n",
    "        identities = torchhd.identity(\n",
    "            num_vectors=num_identity_vectors, dimensions=src.shape[-1], vsa=vsa.value, device=src.device\n",
    "        )\n",
    "        return torch.cat([result, identities])\n",
    "\n",
    "    # Generic fallback: group rows manually in Python (will be slower)\n",
    "    # Currently no support for dim other than0\n",
    "    buckets = [[] for _ in range(dim_size)]\n",
    "    for i, b in enumerate(index.tolist()):\n",
    "        buckets[b].append(src[i])\n",
    "\n",
    "    # initialize output slots\n",
    "    op_hd = torchhd.multibind if op == \"bind\" else torchhd.multibundle\n",
    "    out = []\n",
    "    for bucket in buckets:\n",
    "        if not bucket:\n",
    "            # empty bucket → identity for bind, zero for bundle\n",
    "            identity = type(src).identity(1, src.shape[-1], device=src.device).squeeze(0)\n",
    "            out.append(identity)\n",
    "        else:\n",
    "            # reduce the list by repeatedly applying op\n",
    "            reduced = op_hd(torch.stack(bucket, dim=0))\n",
    "            out.append(reduced)\n",
    "    return torch.stack(out, dim=0)\n",
    "\n",
    "\n",
    "def autograd_test_for_vsa(vsa_name: str, vsa_cls: type):\n",
    "    \"\"\"\n",
    "    For a single VSA type (e.g. MAPTensor or HRRTensor), check that:\n",
    "     - `bind(a, b)` is differentiable.\n",
    "     - `bundle(a, b)` is differentiable.\n",
    "     - `multibind(torch.stack([a,b,c]))` is differentiable.\n",
    "     - `multibundle(torch.stack([a,b,c]))` is differentiable.\n",
    "     - `scatter_hd(src, idx, op=…)` is differentiable.\n",
    "\n",
    "    Returns True if all four ops support autograd without error, False otherwise.\n",
    "    \"\"\"\n",
    "    torch.manual_seed(0)\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "    success = True\n",
    "    dim = 8*8  # very small dimensionality, just for testing\n",
    "\n",
    "    # Create two random float‐valued hypervectors of that VSA class, requiring grad:\n",
    "    # We create them by sampling from .random() or .random_like(...) if available.\n",
    "    try:\n",
    "        # Many torchhd classes have a .random(...) or .random_() factory method.\n",
    "        a = vsa_cls.random(1, dim, requires_grad=True, device=device).squeeze(0)\n",
    "        b = vsa_cls.random(1, dim, requires_grad=True, device=device).squeeze(0)\n",
    "        c = vsa_cls.random(1, dim, requires_grad=True, device=device).squeeze(0)\n",
    "    except AttributeError:\n",
    "        # Fallback: sample floats in {-1, +1} for MAP/HRR, or {0,1} for BSC, then wrap\n",
    "        vals = torch.randint(0, 2, (1, dim), device=device).float() * 2 - 1  # ±1 for MAP/HRR\n",
    "        a = vsa_cls(vals.clone().requires_grad_(True))\n",
    "        b = vsa_cls(vals.clone().requires_grad_(True))\n",
    "        c = vsa_cls(vals.clone().requires_grad_(True))\n",
    "\n",
    "    # Create a dummy “edge_weight” parameter we can optimize\n",
    "    # shape [num_edges, 1]: we pick num_edges=3 so that scatter_hd has 2 buckets\n",
    "    edge_weight = torch.randn(3, 1, requires_grad=True, device=device)\n",
    "\n",
    "    # Prepare dummy “messages” to scatter: shape [3, dim]\n",
    "    # We’ll just stack a, b, c and pretend they are messages for three edges\n",
    "    # (with edge_weight gating).\n",
    "    messages = torch.stack([a, b, c], dim=0)  # but a,b,c are VSA‐specific objects\n",
    "    if isinstance(a, torch.Tensor):\n",
    "        # Already a raw tensor\n",
    "        pass\n",
    "    else:\n",
    "        # If a is an HV wrapper (e.g. MAPTensor), extract its data‐tensor:\n",
    "        messages = torch.stack([a.data, b.data, c.data], dim=0)\n",
    "    # Create an index tensor that lumps indices 0,1→bucket0 and index2→bucket1\n",
    "    idx = torch.tensor([0, 0, 1], dtype=torch.long, device=device)\n",
    "\n",
    "    # 1) Test bind(a,b)\n",
    "    try:\n",
    "        bound = torchhd.bind(a, b)                   #  [oai_citation:4‡torchhd.readthedocs.io](https://torchhd.readthedocs.io/en/stable/torchhd.html?utm_source=chatgpt.com) [oai_citation:5‡torchhd.readthedocs.io](https://torchhd.readthedocs.io/en/stable/getting_started.html?utm_source=chatgpt.com)\n",
    "        if not bound.requires_grad:\n",
    "            bound = bound.requires_grad_()\n",
    "        opt = optim.Adam([bound], lr=1e-2)\n",
    "        out = bound.sum()\n",
    "        out.backward(retain_graph=True)\n",
    "    except Exception as e:\n",
    "        print(f\"[{vsa_name}] bind(...) failed autograd: {e}\")\n",
    "        success = False\n",
    "\n",
    "    # 2) Test bundle(a,b)\n",
    "    try:\n",
    "        bundled = torchhd.bundle(a, b)               #  [oai_citation:6‡torchhd.readthedocs.io](https://torchhd.readthedocs.io/en/stable/torchhd.html?utm_source=chatgpt.com) [oai_citation:7‡torchhd.readthedocs.io](https://torchhd.readthedocs.io/en/stable/getting_started.html?utm_source=chatgpt.com)\n",
    "        if not bundled.requires_grad:\n",
    "            bundled = bundled.requires_grad_()\n",
    "        opt = optim.Adam([bundled], lr=1e-2)\n",
    "        out = bundled.sum()\n",
    "        out.backward(retain_graph=True)\n",
    "    except Exception as e:\n",
    "        print(f\"[{vsa_name}] bundle(...) failed autograd: {e}\")\n",
    "        print(e)\n",
    "        success = False\n",
    "\n",
    "    # 3) Test multibind(torch.stack([a,b,c]))\n",
    "    try:\n",
    "        multi_bound = torchhd.multibind(torch.stack([a, b, c], dim=0))   #  [oai_citation:8‡torchhd.readthedocs.io](https://torchhd.readthedocs.io/en/stable/torchhd.html?utm_source=chatgpt.com) [oai_citation:9‡torchhd.readthedocs.io](https://torchhd.readthedocs.io/en/stable/getting_started.html?utm_source=chatgpt.com)\n",
    "        if not multi_bound.requires_grad:\n",
    "            multi_bound = multi_bound.requires_grad_()\n",
    "        opt = optim.Adam([multi_bound], lr=1e-2)\n",
    "        out = multi_bound.sum()\n",
    "        out.backward(retain_graph=True)\n",
    "    except Exception as e:\n",
    "        print(f\"[{vsa_name}] multibind(...) failed autograd: {e}\")\n",
    "        print(e)\n",
    "        success = False\n",
    "\n",
    "    # 4) Test multibundle(torch.stack([...]))\n",
    "    try:\n",
    "        multi_bundled = torchhd.multibundle(torch.stack([a, b, c], dim=0)) #  [oai_citation:10‡torchhd.readthedocs.io](https://torchhd.readthedocs.io/en/stable/torchhd.html?utm_source=chatgpt.com) [oai_citation:11‡torchhd.readthedocs.io](https://torchhd.readthedocs.io/en/stable/getting_started.html?utm_source=chatgpt.com)\n",
    "        if not multi_bundled.requires_grad:\n",
    "            multi_bundled = multi_bundled.requires_grad_()\n",
    "        opt = optim.Adam([multi_bundled], lr=1e-2)\n",
    "        out = multi_bundled.sum()\n",
    "        out.backward(retain_graph=True)\n",
    "    except Exception as e:\n",
    "        print(f\"[{vsa_name}] multibundle(...) failed autograd: {e}\")\n",
    "        print(e)\n",
    "        success = False\n",
    "\n",
    "    # 5) Test scatter_hd(messages, idx, op=\"bind\")\n",
    "    try:\n",
    "        # Note: scatter_hd expects HV‐typed inputs, so wrap messages back to vsa\n",
    "        hv_messages = vsa_cls(messages.clone())\n",
    "        bucketed = scatter_hd(hv_messages, idx, op=\"bind\", dim_size=2)  #  [oai_citation:12‡torchhd.readthedocs.io](https://torchhd.readthedocs.io/en/stable/torchhd.html?utm_source=chatgpt.com) [oai_citation:13‡torchhd.readthedocs.io](https://torchhd.readthedocs.io/en/stable/getting_started.html?utm_source=chatgpt.com)\n",
    "        # bucketed has shape [2, dim]\n",
    "        # To test gradient, we artificially add and sum:\n",
    "        logits = bucketed.sum(dim=1, keepdim=True).sum()\n",
    "        # Create a dummy optimizer on the logits (it will backprop through bucketed and thus through edge_weight)\n",
    "        opt = optim.Adam([logits.requires_grad_(True)], lr=1e-2)\n",
    "        out = logits * edge_weight.sum()  # force linkage to edge_weight\n",
    "        out.backward(retain_graph=True)\n",
    "    except Exception as e:\n",
    "        print(f\"[{vsa_name}] scatter_hd(..., op='bind') failed autograd: {e}\")\n",
    "        print(e)\n",
    "        success = False\n",
    "\n",
    "    # 6) Test scatter_hd(messages, idx, op=\"bundle\")\n",
    "    try:\n",
    "        hv_messages = vsa_cls(messages.clone())\n",
    "        bucketed2 = scatter_hd(hv_messages, idx, op=\"bundle\", dim_size=2)  #  [oai_citation:14‡torchhd.readthedocs.io](https://torchhd.readthedocs.io/en/stable/torchhd.html?utm_source=chatgpt.com) [oai_citation:15‡torchhd.readthedocs.io](https://torchhd.readthedocs.io/en/stable/getting_started.html?utm_source=chatgpt.com)\n",
    "        logits2 = bucketed2.sum(dim=1, keepdim=True).sum()\n",
    "        opt = optim.Adam([logits2.requires_grad_(True)], lr=1e-2)\n",
    "        out2 = logits2 * edge_weight.sum()\n",
    "        out2.backward(retain_graph=True)\n",
    "    except Exception as e:\n",
    "        print(f\"[{vsa_name}] scatter_hd(..., op='bundle') failed autograd: {e}\")\n",
    "        print(e)\n",
    "        success = False\n",
    "\n",
    "    return success\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"=== TorchHD Autograd Compatibility Check ===\\n\")\n",
    "    all_passed = True\n",
    "    for name, cls in VSA_TYPES.items():\n",
    "        ok = autograd_test_for_vsa(name, cls)\n",
    "        print(f\"{name} autograd pass: {ok}\")\n",
    "        if not ok:\n",
    "            all_passed = False\n",
    "    if all_passed:\n",
    "        print(\"\\nALL TorchHD ops support autograd for all tested VSA types.\")\n",
    "    else:\n",
    "        print(\"\\nSOME TorchHD ops failed autograd. See the messages above.\")\n",
    "\n",
    "main()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TorchHD Autograd Compatibility Check ===\n",
      "\n",
      "[MAP] bind(...) failed autograd: can't optimize a non-leaf Tensor\n",
      "[MAP] bundle(...) failed autograd: can't optimize a non-leaf Tensor\n",
      "can't optimize a non-leaf Tensor\n",
      "[MAP] multibind(...) failed autograd: can't optimize a non-leaf Tensor\n",
      "can't optimize a non-leaf Tensor\n",
      "[MAP] multibundle(...) failed autograd: can't optimize a non-leaf Tensor\n",
      "can't optimize a non-leaf Tensor\n",
      "[MAP] scatter_hd(..., op='bind') failed autograd: can't optimize a non-leaf Tensor\n",
      "can't optimize a non-leaf Tensor\n",
      "[MAP] scatter_hd(..., op='bundle') failed autograd: can't optimize a non-leaf Tensor\n",
      "can't optimize a non-leaf Tensor\n",
      "MAP autograd pass: False\n",
      "[HRR] bind(...) failed autograd: can't optimize a non-leaf Tensor\n",
      "[HRR] bundle(...) failed autograd: can't optimize a non-leaf Tensor\n",
      "can't optimize a non-leaf Tensor\n",
      "[HRR] multibind(...) failed autograd: can't optimize a non-leaf Tensor\n",
      "can't optimize a non-leaf Tensor\n",
      "[HRR] multibundle(...) failed autograd: can't optimize a non-leaf Tensor\n",
      "can't optimize a non-leaf Tensor\n",
      "[HRR] scatter_hd(..., op='bind') failed autograd: can't optimize a non-leaf Tensor\n",
      "can't optimize a non-leaf Tensor\n",
      "[HRR] scatter_hd(..., op='bundle') failed autograd: can't optimize a non-leaf Tensor\n",
      "can't optimize a non-leaf Tensor\n",
      "HRR autograd pass: False\n",
      "[FHRR] bind(...) failed autograd: can't optimize a non-leaf Tensor\n",
      "[FHRR] bundle(...) failed autograd: can't optimize a non-leaf Tensor\n",
      "can't optimize a non-leaf Tensor\n",
      "[FHRR] multibind(...) failed autograd: can't optimize a non-leaf Tensor\n",
      "can't optimize a non-leaf Tensor\n",
      "[FHRR] multibundle(...) failed autograd: can't optimize a non-leaf Tensor\n",
      "can't optimize a non-leaf Tensor\n",
      "[FHRR] scatter_hd(..., op='bind') failed autograd: can't optimize a non-leaf Tensor\n",
      "can't optimize a non-leaf Tensor\n",
      "[FHRR] scatter_hd(..., op='bundle') failed autograd: can't optimize a non-leaf Tensor\n",
      "can't optimize a non-leaf Tensor\n",
      "FHRR autograd pass: False\n",
      "[VTB] bind(...) failed autograd: can't optimize a non-leaf Tensor\n",
      "[VTB] bundle(...) failed autograd: can't optimize a non-leaf Tensor\n",
      "can't optimize a non-leaf Tensor\n",
      "[VTB] multibind(...) failed autograd: can't optimize a non-leaf Tensor\n",
      "can't optimize a non-leaf Tensor\n",
      "[VTB] multibundle(...) failed autograd: can't optimize a non-leaf Tensor\n",
      "can't optimize a non-leaf Tensor\n",
      "[VTB] scatter_hd(..., op='bind') failed autograd: stack expects each tensor to be equal size, but got [64] at entry 0 and [1, 1, 64] at entry 1\n",
      "stack expects each tensor to be equal size, but got [64] at entry 0 and [1, 1, 64] at entry 1\n",
      "[VTB] scatter_hd(..., op='bundle') failed autograd: can't optimize a non-leaf Tensor\n",
      "can't optimize a non-leaf Tensor\n",
      "VTB autograd pass: False\n",
      "\n",
      "SOME TorchHD ops failed autograd. See the messages above.\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T14:35:20.740637Z",
     "start_time": "2025-06-03T14:35:20.736028Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torchhd\n",
    "from torchhd.tensors.map import MAPTensor\n",
    "\n",
    "# 1) Create two _leaf_ MAP hypervectors:\n",
    "leaf_a = MAPTensor.random(1, 64, requires_grad=True).squeeze(0)  # leaf\n",
    "leaf_b = MAPTensor.random(1, 64, requires_grad=True).squeeze(0)  # leaf\n",
    "\n",
    "# 2) Bind them → result is non-leaf\n",
    "bound = torchhd.bind(leaf_a, leaf_b)\n",
    "print(bound.is_leaf)       # False\n",
    "print(bound.grad_fn)       # e.g. <AsStridedBackward0 object at 0x...>\n",
    "\n",
    "# 3) Bundle them → result is non-leaf\n",
    "bundled = torchhd.bundle(leaf_a, leaf_b)\n",
    "print(bundled.is_leaf)     # False\n",
    "print(bundled.grad_fn)     # e.g. <AliasBackward0 object at 0x...>\n",
    "\n",
    "# 4) Now, propose a simple “loss” and call backward:\n",
    "loss = (bound + bundled).sum()\n",
    "loss.backward()            # No error! a.grad and b.grad populated"
   ],
   "id": "1f2efc795d275940",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "<AliasBackward0 object at 0x1498f90f0>\n",
      "False\n",
      "<AliasBackward0 object at 0x1498fa260>\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T14:36:11.860843Z",
     "start_time": "2025-06-03T14:36:11.856290Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Continuing from above: leaf_a, leaf_b, but add leaf_c\n",
    "leaf_c = MAPTensor.random(1, 64, requires_grad=True).squeeze(0)  # leaf\n",
    "\n",
    "# 1) multibind: bind multiple leaves at once → non-leaf\n",
    "multi_bound = torchhd.multibind(torch.stack([leaf_a, leaf_b, leaf_c], dim=0))\n",
    "print(multi_bound.is_leaf)      # False\n",
    "print(multi_bound.grad_fn)      # e.g. <AddmmBackward0 object at 0x...> (depends on implementation)\n",
    "\n",
    "# 2) multibundle: bundle multiple leaves at once → non-leaf\n",
    "multi_bundled = torchhd.multibundle(torch.stack([leaf_a, leaf_b, leaf_c], dim=0))\n",
    "print(multi_bundled.is_leaf)    # False\n",
    "print(multi_bundled.grad_fn)    # e.g. <SumBackward0 object at 0x...>\n",
    "\n",
    "# 3) Backprop through them\n",
    "loss2 = (multi_bound + multi_bundled).sum()\n",
    "loss2.backward()  # Works fine; leaf_a.grad, leaf_b.grad, leaf_c.grad all non-null"
   ],
   "id": "8724a2f2aee98c6e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "<AliasBackward0 object at 0x1498f9a80>\n",
      "False\n",
      "<AliasBackward0 object at 0x1498f9c30>\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T14:36:47.586609Z",
     "start_time": "2025-06-03T14:36:47.580629Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Suppose we have three leaf hypervectors (for edges) and a leaf logit parameter:\n",
    "leaf_a = MAPTensor.random(1, 64, requires_grad=True).squeeze(0)\n",
    "leaf_b = MAPTensor.random(1, 64, requires_grad=True).squeeze(0)\n",
    "leaf_c = MAPTensor.random(1, 64, requires_grad=True).squeeze(0)\n",
    "edge_logits = torch.randn(3, 1, requires_grad=True)   # leaf parameter\n",
    "\n",
    "# Stack them into a “messages” tensor\n",
    "msgs = torch.stack([leaf_a.data, leaf_b.data, leaf_c.data], dim=0)  # raw Tensor [3,64]\n",
    "hv_msgs = MAPTensor(msgs)       # wrapping back into a MAPTensor (non-leaf)\n",
    "\n",
    "# Indices to scatter into 2 buckets: first two into bucket0, last into bucket1\n",
    "idx = torch.tensor([0, 0, 1], dtype=torch.long)\n",
    "\n",
    "# 1) Scatter‐HD with bind\n",
    "bucketed_bind = scatter_hd(hv_msgs, idx, op=\"bind\", dim_size=2)\n",
    "# bucketed_bind is a MAPTensor of shape [2, 64], non‐leaf\n",
    "\n",
    "# 2) Form a scalar “loss” that depends on both bucketed_bind and edge_logits:\n",
    "out = (bucketed_bind.sum(dim=-1).sum() * edge_logits.sum())\n",
    "out.backward()  # No error → edge_logits.grad is non-null\n",
    "\n",
    "# 3) Similarly, for bundle\n",
    "bucketed_bundle = scatter_hd(hv_msgs, idx, op=\"bundle\", dim_size=2)\n",
    "out2 = (bucketed_bundle.sum(dim=-1).sum() * edge_logits.sum())\n",
    "out2.backward()  # Works, edge_logits.grad accumulates"
   ],
   "id": "7f2cc500f666c685",
   "outputs": [],
   "execution_count": 17
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
